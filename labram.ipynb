{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee6a7264",
   "metadata": {},
   "source": [
    "# EEGMMIDB — Data Exploration & NeuroGPT Classification\n",
    "\n",
    "Goal:\n",
    "- Load the EEGMMIDB motor movement / imagery dataset\n",
    "- Explore signals and labels\n",
    "- Classification with NeuroGPT usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b813e51b-f23c-4999-9a39-bec3cbbe821f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mne\n",
      "  Using cached mne-1.11.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting accelerate>=0.26.0\n",
      "  Using cached accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting tf-keras\n",
      "  Using cached tf_keras-2.20.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: transformers[torch] in /opt/micromamba/lib/python3.11/site-packages (4.56.2)\n",
      "Requirement already satisfied: decorator in /opt/micromamba/lib/python3.11/site-packages (from mne) (5.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/micromamba/lib/python3.11/site-packages (from mne) (3.1.6)\n",
      "Requirement already satisfied: lazy-loader>=0.3 in /opt/micromamba/lib/python3.11/site-packages (from mne) (0.4)\n",
      "Requirement already satisfied: matplotlib>=3.8 in /opt/micromamba/lib/python3.11/site-packages (from mne) (3.10.6)\n",
      "Requirement already satisfied: numpy<3,>=1.26 in /opt/micromamba/lib/python3.11/site-packages (from mne) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/micromamba/lib/python3.11/site-packages (from mne) (25.0)\n",
      "Requirement already satisfied: pooch>=1.5 in /opt/micromamba/lib/python3.11/site-packages (from mne) (1.8.2)\n",
      "Requirement already satisfied: scipy>=1.11 in /opt/micromamba/lib/python3.11/site-packages (from mne) (1.16.2)\n",
      "Requirement already satisfied: tqdm in /opt/micromamba/lib/python3.11/site-packages (from mne) (4.67.1)\n",
      "Requirement already satisfied: filelock in /opt/micromamba/lib/python3.11/site-packages (from transformers[torch]) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/micromamba/lib/python3.11/site-packages (from transformers[torch]) (0.35.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/micromamba/lib/python3.11/site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/micromamba/lib/python3.11/site-packages (from transformers[torch]) (2025.9.18)\n",
      "Requirement already satisfied: requests in /opt/micromamba/lib/python3.11/site-packages (from transformers[torch]) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/micromamba/lib/python3.11/site-packages (from transformers[torch]) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/micromamba/lib/python3.11/site-packages (from transformers[torch]) (0.6.2)\n",
      "Requirement already satisfied: torch>=2.2 in /opt/micromamba/lib/python3.11/site-packages (from transformers[torch]) (2.8.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/micromamba/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/micromamba/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/micromamba/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (1.1.10)\n",
      "Requirement already satisfied: psutil in /opt/micromamba/lib/python3.11/site-packages (from accelerate>=0.26.0) (6.1.1)\n",
      "Requirement already satisfied: tensorflow<2.21,>=2.20 in /opt/micromamba/lib/python3.11/site-packages (from tf-keras) (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/micromamba/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/micromamba/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/micromamba/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/micromamba/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in /opt/micromamba/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/micromamba/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in /opt/micromamba/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in /opt/micromamba/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (6.31.1)\n",
      "Requirement already satisfied: setuptools in /opt/micromamba/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/micromamba/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/micromamba/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/micromamba/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/micromamba/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.62.2)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in /opt/micromamba/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in /opt/micromamba/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.11.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/micromamba/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.14.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /opt/micromamba/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/micromamba/lib/python3.11/site-packages (from requests->transformers[torch]) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/micromamba/lib/python3.11/site-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/micromamba/lib/python3.11/site-packages (from requests->transformers[torch]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/micromamba/lib/python3.11/site-packages (from requests->transformers[torch]) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/micromamba/lib/python3.11/site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.9)\n",
      "Requirement already satisfied: pillow in /opt/micromamba/lib/python3.11/site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (11.3.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/micromamba/lib/python3.11/site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/micromamba/lib/python3.11/site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/micromamba/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow<2.21,>=2.20->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in /opt/micromamba/lib/python3.11/site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (14.1.0)\n",
      "Requirement already satisfied: namex in /opt/micromamba/lib/python3.11/site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.0)\n",
      "Requirement already satisfied: optree in /opt/micromamba/lib/python3.11/site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.17.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/micromamba/lib/python3.11/site-packages (from matplotlib>=3.8->mne) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/micromamba/lib/python3.11/site-packages (from matplotlib>=3.8->mne) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/micromamba/lib/python3.11/site-packages (from matplotlib>=3.8->mne) (4.60.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/micromamba/lib/python3.11/site-packages (from matplotlib>=3.8->mne) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/micromamba/lib/python3.11/site-packages (from matplotlib>=3.8->mne) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/micromamba/lib/python3.11/site-packages (from matplotlib>=3.8->mne) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/micromamba/lib/python3.11/site-packages (from pooch>=1.5->mne) (4.4.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->transformers[torch]) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->transformers[torch]) (3.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->transformers[torch]) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->transformers[torch]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->transformers[torch]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->transformers[torch]) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->transformers[torch]) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->transformers[torch]) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->transformers[torch]) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->transformers[torch]) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->transformers[torch]) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->transformers[torch]) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->transformers[torch]) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->transformers[torch]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->transformers[torch]) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->transformers[torch]) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->transformers[torch]) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/micromamba/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.2->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/micromamba/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/micromamba/lib/python3.11/site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/micromamba/lib/python3.11/site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/micromamba/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.2)\n",
      "Using cached mne-1.11.0-py3-none-any.whl (7.5 MB)\n",
      "Using cached accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Using cached tf_keras-2.20.1-py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: mne, accelerate, tf-keras\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [tf-keras]2/3\u001b[0m [tf-keras]e]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.12.0 mne-1.11.0 tf-keras-2.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install mne \"transformers[torch]\" \"accelerate>=0.26.0\" tf-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374e6ab2-f2b6-4073-bbcc-9e25ed302134",
   "metadata": {},
   "source": [
    "## PhysioNet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb62aefd-3ec5-4a4d-93cd-37ca32a82f8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src.labram_dataloader'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     10\u001b[39m DATA_PATH = PROJECT_ROOT / \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mphysionet.org\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mfiles\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33meegmmidb\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33m1.0.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m sys.path.append(\u001b[38;5;28mstr\u001b[39m(SRC_PATH))\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlabram_dataloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EEGMMIDBDataset\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mProject root:\u001b[39m\u001b[33m\"\u001b[39m, PROJECT_ROOT)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mData path:\u001b[39m\u001b[33m\"\u001b[39m, DATA_PATH)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src.labram_dataloader'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Project paths\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "SRC_PATH = PROJECT_ROOT / \"src\"\n",
    "DATA_PATH = PROJECT_ROOT / \"data\" / \"physionet.org\" / \"files\" / \"eegmmidb\" / \"1.0.0\"\n",
    "\n",
    "sys.path.append(str(SRC_PATH))\n",
    "\n",
    "from src.labram_dataloader import EEGMMIDBDataset\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "print(\"Data path:\", DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92f5272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "\n",
    "edf = DATA_PATH / \"S001\" / \"S001R03.edf\"\n",
    "raw = mne.io.read_raw_edf(edf, preload=False, verbose=False)\n",
    "\n",
    "print(raw.ch_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f0c892",
   "metadata": {},
   "source": [
    "### Load Dataset for exploration\n",
    "\n",
    "Subject 1, motor execution + imagery, left/right hand\n",
    "\n",
    "Classes:\n",
    "- 0: Right imagined\n",
    "- 1: Right real\n",
    "- 2: Left imagined\n",
    "- 3: Left real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a86bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EEGMMIDBDataset(\n",
    "    root_path=str(DATA_PATH),\n",
    "    subjects=[1],\n",
    "    runs=[4, 8, 12],\n",
    "    t_min=0.0,\n",
    "    t_max=4.0,\n",
    "    normalization=True\n",
    ")\n",
    "\n",
    "print(\"Total extracted trials:\", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfadd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "labels = [dataset[i][\"labels\"].item() for i in range(len(dataset))]\n",
    "\n",
    "label_map = {\n",
    "    0: \"Left Imagined\",\n",
    "    1: \"Right Imagined\",\n",
    "}\n",
    "\n",
    "counts = Counter(labels)\n",
    "\n",
    "print(\"Dataset-level class counts:\")\n",
    "for k in sorted(label_map):\n",
    "    print(f\"{label_map[k]:<15}: {counts.get(k, 0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6d0642-bca0-4a09-bba3-4012d8ea0086",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792e70e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from collections import defaultdict\n",
    "\n",
    "event_summary = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for run in [4, 8, 12]:\n",
    "    edf_path = DATA_PATH / \"S001\" / f\"S001R{run:02d}.edf\"\n",
    "    raw = mne.io.read_raw_edf(edf_path, preload=False, verbose=False)\n",
    "    \n",
    "    _, event_id = mne.events_from_annotations(raw, verbose=False)\n",
    "    \n",
    "    for label in event_id.keys():\n",
    "        event_code = label.split(\"/\")[-1]\n",
    "        event_summary[run][event_code] += 1\n",
    "\n",
    "print(\"Raw EDF annotation summary (Subject 1):\\n\")\n",
    "for run, events in event_summary.items():\n",
    "    print(f\"Run {run}: {dict(events)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9439ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample-wise label sanity check (first 10 trials):\\n\")\n",
    "\n",
    "for i in range(10):\n",
    "    sample = dataset[i]\n",
    "    label = sample[\"labels\"].item()\n",
    "    print(f\"Trial {i:02d} -> Class {label}: {label_map[label]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6678bb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "energy_by_class = defaultdict(list)\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    sample = dataset[i]\n",
    "    label = sample[\"labels\"].item()\n",
    "    energy = sample[\"inputs\"].pow(2).mean().item()\n",
    "    energy_by_class[label].append(energy)\n",
    "\n",
    "print(\"Mean signal energy per class:\")\n",
    "for k in sorted(label_map):\n",
    "    print(f\"{label_map[k]:<15}: {np.mean(energy_by_class[k]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d88cd98",
   "metadata": {},
   "source": [
    "### NeuroGPT Teaser\n",
    "\n",
    "Each trial is already:\n",
    "- 22-channel (NeuroGPT format)\n",
    "- Normalized\n",
    "- Fixed-length\n",
    "- Labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a9b164",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = dataset[0]\n",
    "print(batch[\"inputs\"].shape)\n",
    "print(batch[\"labels\"].item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3daa241",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed89fab4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd0c63cc",
   "metadata": {},
   "source": [
    "### Loading the train, test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8386c110",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subjects = list(range(1, 109))  # subject range\n",
    "print(\"Subjects:\", all_subjects)\n",
    "\n",
    "MI_ME_RUNS = [4, 8, 12]\n",
    "print(\"MI_ME runs:\", MI_ME_RUNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2db9cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folds\n",
    "\n",
    "train_folds = []\n",
    "test_folds = []\n",
    "\n",
    "for i in range(len(all_subjects) // 2):\n",
    "    test_subjects = all_subjects[i*2 : i*2+2]\n",
    "    train_subjects = all_subjects[:i*2] + all_subjects[i*2+2:]\n",
    "\n",
    "    train_folds.append(train_subjects)\n",
    "    test_folds.append(test_subjects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9fb3485-15d1-4ac6-a339-e28fd8ef63a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n"
     ]
    }
   ],
   "source": [
    "from src.labram_dataloader import EEGMMIDBDataset\n",
    "\n",
    "\"\"\"\n",
    "train_dataset = EEGMMIDBDataset(\n",
    "    root_path=str(DATA_PATH),\n",
    "    subjects=train_subjects,\n",
    "    runs=MI_ME_RUNS,\n",
    "    t_min=0.0,\n",
    "    t_max=4.0,\n",
    "    normalization=True\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "test_dataset_EEG = EEGMMIDBDataset(\n",
    "    root_path=str(DATA_PATH),\n",
    "    subjects=test_subjects,\n",
    "    runs=MI_ME_RUNS,\n",
    "    t_min=0.0,\n",
    "    t_max=4.0,\n",
    "    normalization=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "907856ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test trials : 90\n",
      "Input shape: torch.Size([64, 1600])\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "#print(\"Train trials:\", len(train_dataset))\n",
    "print(\"Test trials :\", len(test_dataset_EEG))\n",
    "\n",
    "batch = test_dataset_EEG[0]\n",
    "print(\"Input shape:\", batch[\"inputs\"].shape)   # (22, T)\n",
    "print(\"Label:\", batch[\"labels\"].item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267da58c-ef7a-4478-9083-617fd2014b99",
   "metadata": {},
   "source": [
    "## BCI Datatset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c947d26c-3d05-40a2-9fa7-f59ad6b6cb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "home = os.environ[\"HOME\"]\n",
    "python_imports = f\"{home}/shared/DLN-2025W/notebook-eeg/Lab_GPT_based_EEG_decoder\"\n",
    "cache_root = f\"{home}/shared/DLN-2025W/notebook-eeg/Lab_GPT_based_EEG_decoder/\"\n",
    "sys.path.append(python_imports)\n",
    "\n",
    "with open(os.path.join(\"NeuroGPT_mini/config.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4df973ee-057b-48da-b817-248b1a0ae973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import sys \n",
    "sys.path.insert(0,os.path.join('NeuroGPT_mini/') )\n",
    "from sklearn.metrics import balanced_accuracy_score, cohen_kappa_score, f1_score\n",
    "import random\n",
    "import json\n",
    "\n",
    "### Import related to Transformer model (from files located in /NeuroGPT directory)\n",
    "\n",
    "from encoder.conformer_braindecode import EEGConformer\n",
    "from decoder.make_decoder import make_decoder\n",
    "from embedder.make import make_embedder\n",
    "from trainer.make import make_trainer\n",
    "from trainer.base import Trainer\n",
    "from decoder.unembedder import make_unembedder\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a848704-1022-4e0b-83b5-f781d34abf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BCIdataloader.labram_MIdataset import MotorImageryDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb045469-882c-4c44-9eed-1973a25d2f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = {\"dst_data_path\" : os.path.join(python_imports, \"bciiv2a_eeg_npz/\"),\n",
    "         \"pretrained_model\" : os.path.join(\"NeuroGPT_mini/pytorch_model.bin\"), \n",
    "         \"log_dir\" :os.path.join(python_imports,\"training_logs/\")}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a8b72ff-6306-47cf-a75c-8353ace22e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files from all the subjects :  ['A01E.npz', 'A01T.npz', 'A02E.npz', 'A02T.npz', 'A03E.npz', 'A03T.npz', 'A04E.npz', 'A04T.npz', 'A05E.npz', 'A05T.npz', 'A06E.npz', 'A06T.npz', 'A07E.npz', 'A07T.npz', 'A08E.npz', 'A08T.npz', 'A09E.npz', 'A09T.npz']\n",
      "Train dataset : \n",
      "['A01E.npz', 'A01T.npz', 'A02E.npz', 'A02T.npz', 'A03E.npz', 'A03T.npz', 'A04E.npz', 'A04T.npz', 'A05E.npz', 'A05T.npz', 'A06E.npz', 'A06T.npz', 'A07E.npz', 'A07T.npz', 'A08E.npz', 'A08T.npz']\n",
      "Test dataset :\n",
      "['A09E.npz', 'A09T.npz']\n",
      "Input shape: torch.Size([64, 1600])\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "# Compute the dataset (train, test, validation) \n",
    "downstream_path = config_path[\"dst_data_path\"]\n",
    "filenames = sorted(os.listdir(downstream_path))[:18]\n",
    "print(\"Files from all the subjects : \", filenames)\n",
    "train_folds = []\n",
    "test_folds = []\n",
    "\n",
    "for i in range(9):\n",
    "    train_files = filenames[0:i*2] + filenames[i*2+2:]\n",
    "    test_files = filenames[i*2 : i*2+2]\n",
    "\n",
    "\n",
    "print(\"Train dataset : \")\n",
    "print(train_files)\n",
    "\n",
    "\n",
    "train_dataset = MotorImageryDataset(train_files, root_path=downstream_path)\n",
    "\n",
    "\n",
    "print(\"Test dataset :\")\n",
    "print(test_files)\n",
    "\n",
    "# On the way, compute the test dataset as :\n",
    "\n",
    "## TO DO : instanciate a MotorImageryDataset object but with the testing files\n",
    "# test_dataset = ... \n",
    "# YOUR CODE HERE\n",
    "validation_dataset = MotorImageryDataset(test_files, root_path=downstream_path)\n",
    "\n",
    "batch = train_dataset[0]\n",
    "print(\"Input shape:\", batch[\"inputs\"].shape)   # (22, T)\n",
    "print(\"Label:\", batch[\"labels\"].item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2027a508-8cb8-497d-8922-27307e101892",
   "metadata": {},
   "source": [
    "## Compare and align datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d46e06a-07b3-40d4-a1f5-9a56b8c607b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset Alignment Report ===\n",
      "\n",
      "[Shape Check]\n",
      "BCI Shape:    [64, 1600]\n",
      "EEGMMI Shape: [64, 1600]\n",
      "✅ Shapes match.\n",
      "\n",
      "[Statistics Check]\n",
      "BCI Mean: -0.0000 | Std: 0.4635\n",
      "EEG Mean: 0.0000 | Std: 0.4635\n",
      "\n",
      "[Channel/Projection Check]\n",
      "BCI Number of Channels: 64\n",
      "EEG Number of Channels: 64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compare_datasets(bci_ds, eegmmi_ds):\n",
    "    # 1. Pull one sample\n",
    "    bci_sample = bci_ds[0]\n",
    "    eeg_sample = eegmmi_ds[0]\n",
    "    \n",
    "    # 2. Extract data\n",
    "    bci_data = bci_sample[\"inputs\"]\n",
    "    eeg_data = eeg_sample[\"inputs\"]\n",
    "    \n",
    "    print(\"=== Dataset Alignment Report ===\")\n",
    "    \n",
    "    # Check Shapes\n",
    "    print(f\"\\n[Shape Check]\")\n",
    "    print(f\"BCI Shape:    {list(bci_data.shape)}\")\n",
    "    print(f\"EEGMMI Shape: {list(eeg_data.shape)}\")\n",
    "    if bci_data.shape == eeg_data.shape:\n",
    "        print(\"✅ Shapes match.\")\n",
    "    else:\n",
    "        print(\"❌ SHAPE MISMATCH! The model will crash.\")\n",
    "\n",
    "    # Check Statistics (Scaling)\n",
    "    print(f\"\\n[Statistics Check]\")\n",
    "    print(f\"BCI Mean: {bci_data.mean().item():.4f} | Std: {bci_data.std().item():.4f}\")\n",
    "    print(f\"EEG Mean: {eeg_data.mean().item():.4f} | Std: {eeg_data.std().item():.4f}\")\n",
    "    \n",
    "    # Check Channels (Projection)\n",
    "    print(f\"\\n[Channel/Projection Check]\")\n",
    "    print(f\"BCI Number of Channels: {bci_data.shape[-2]}\")\n",
    "    print(f\"EEG Number of Channels: {eeg_data.shape[-2]}\")\n",
    "# Run the comparison\n",
    "# (Assuming you've initialized 'train_dataset' and 'test_dataset')\n",
    "compare_datasets(train_dataset, validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "127dbc85-ad86-4c04-bbf7-39564e19af44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Label Distribution: Counter({1: 576, 0: 576})\n",
      "Test Label Distribution: Counter({1: 72, 0: 72})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Check Training Labels\n",
    "train_counts = Counter(train_dataset.labels)\n",
    "print(f\"Training Label Distribution: {train_counts}\")\n",
    "# Expected: {0: count_left, 1: count_right}\n",
    "\n",
    "# Check Test Labels\n",
    "test_counts = Counter(validation_dataset.labels)\n",
    "print(f\"Test Label Distribution: {test_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f62bd51-b77f-46cb-a25f-5f70163658a7",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cbd4366-3722-4a24-a3d9-51e92a40353a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: braindecode in /opt/micromamba/lib/python3.11/site-packages (1.3.2)\n",
      "Requirement already satisfied: torch>=2.2 in /opt/micromamba/lib/python3.11/site-packages (from braindecode) (2.8.0)\n",
      "Requirement already satisfied: torchaudio>=2.0 in /opt/micromamba/lib/python3.11/site-packages (from braindecode) (2.8.0)\n",
      "Requirement already satisfied: mne>=1.11.0 in /opt/micromamba/lib/python3.11/site-packages (from braindecode) (1.11.0)\n",
      "Requirement already satisfied: mne_bids>=0.18 in /opt/micromamba/lib/python3.11/site-packages (from braindecode) (0.18.0)\n",
      "Requirement already satisfied: h5py in /opt/micromamba/lib/python3.11/site-packages (from braindecode) (3.14.0)\n",
      "Requirement already satisfied: skorch>=1.3.0 in /opt/micromamba/lib/python3.11/site-packages (from braindecode) (1.3.1)\n",
      "Requirement already satisfied: joblib in /opt/micromamba/lib/python3.11/site-packages (from braindecode) (1.5.2)\n",
      "Requirement already satisfied: torchinfo in /opt/micromamba/lib/python3.11/site-packages (from braindecode) (1.8.0)\n",
      "Requirement already satisfied: wfdb in /opt/micromamba/lib/python3.11/site-packages (from braindecode) (4.3.0)\n",
      "Requirement already satisfied: linear_attention_transformer in /opt/micromamba/lib/python3.11/site-packages (from braindecode) (0.19.1)\n",
      "Requirement already satisfied: docstring_inheritance in /opt/micromamba/lib/python3.11/site-packages (from braindecode) (3.0.0)\n",
      "Requirement already satisfied: rotary_embedding_torch in /opt/micromamba/lib/python3.11/site-packages (from braindecode) (0.8.9)\n",
      "Requirement already satisfied: decorator in /opt/micromamba/lib/python3.11/site-packages (from mne>=1.11.0->braindecode) (5.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/micromamba/lib/python3.11/site-packages (from mne>=1.11.0->braindecode) (3.1.6)\n",
      "Requirement already satisfied: lazy-loader>=0.3 in /opt/micromamba/lib/python3.11/site-packages (from mne>=1.11.0->braindecode) (0.4)\n",
      "Requirement already satisfied: matplotlib>=3.8 in /opt/micromamba/lib/python3.11/site-packages (from mne>=1.11.0->braindecode) (3.10.6)\n",
      "Requirement already satisfied: numpy<3,>=1.26 in /opt/micromamba/lib/python3.11/site-packages (from mne>=1.11.0->braindecode) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/micromamba/lib/python3.11/site-packages (from mne>=1.11.0->braindecode) (25.0)\n",
      "Requirement already satisfied: pooch>=1.5 in /opt/micromamba/lib/python3.11/site-packages (from mne>=1.11.0->braindecode) (1.8.2)\n",
      "Requirement already satisfied: scipy>=1.11 in /opt/micromamba/lib/python3.11/site-packages (from mne>=1.11.0->braindecode) (1.16.2)\n",
      "Requirement already satisfied: tqdm in /opt/micromamba/lib/python3.11/site-packages (from mne>=1.11.0->braindecode) (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/micromamba/lib/python3.11/site-packages (from matplotlib>=3.8->mne>=1.11.0->braindecode) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/micromamba/lib/python3.11/site-packages (from matplotlib>=3.8->mne>=1.11.0->braindecode) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/micromamba/lib/python3.11/site-packages (from matplotlib>=3.8->mne>=1.11.0->braindecode) (4.60.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/micromamba/lib/python3.11/site-packages (from matplotlib>=3.8->mne>=1.11.0->braindecode) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /opt/micromamba/lib/python3.11/site-packages (from matplotlib>=3.8->mne>=1.11.0->braindecode) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/micromamba/lib/python3.11/site-packages (from matplotlib>=3.8->mne>=1.11.0->braindecode) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/micromamba/lib/python3.11/site-packages (from matplotlib>=3.8->mne>=1.11.0->braindecode) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/micromamba/lib/python3.11/site-packages (from pooch>=1.5->mne>=1.11.0->braindecode) (4.4.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/micromamba/lib/python3.11/site-packages (from pooch>=1.5->mne>=1.11.0->braindecode) (2.32.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/micromamba/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.8->mne>=1.11.0->braindecode) (1.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/micromamba/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.11.0->braindecode) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/micromamba/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.11.0->braindecode) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/micromamba/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.11.0->braindecode) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/micromamba/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.11.0->braindecode) (2025.8.3)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in /opt/micromamba/lib/python3.11/site-packages (from skorch>=1.3.0->braindecode) (1.7.2)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /opt/micromamba/lib/python3.11/site-packages (from skorch>=1.3.0->braindecode) (0.9.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/micromamba/lib/python3.11/site-packages (from scikit-learn>=0.22.0->skorch>=1.3.0->braindecode) (3.6.0)\n",
      "Requirement already satisfied: filelock in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->braindecode) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->braindecode) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->braindecode) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->braindecode) (3.5)\n",
      "Requirement already satisfied: fsspec in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->braindecode) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->braindecode) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->braindecode) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->braindecode) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->braindecode) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->braindecode) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->braindecode) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->braindecode) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->braindecode) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->braindecode) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->braindecode) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->braindecode) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->braindecode) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->braindecode) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->braindecode) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /opt/micromamba/lib/python3.11/site-packages (from torch>=2.2->braindecode) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/micromamba/lib/python3.11/site-packages (from triton==3.4.0->torch>=2.2->braindecode) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/micromamba/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.2->braindecode) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/micromamba/lib/python3.11/site-packages (from jinja2->mne>=1.11.0->braindecode) (3.0.2)\n",
      "Requirement already satisfied: axial-positional-embedding in /opt/micromamba/lib/python3.11/site-packages (from linear_attention_transformer->braindecode) (0.3.12)\n",
      "Requirement already satisfied: einops in /opt/micromamba/lib/python3.11/site-packages (from linear_attention_transformer->braindecode) (0.8.1)\n",
      "Requirement already satisfied: linformer>=0.1.0 in /opt/micromamba/lib/python3.11/site-packages (from linear_attention_transformer->braindecode) (0.2.3)\n",
      "Requirement already satisfied: local-attention in /opt/micromamba/lib/python3.11/site-packages (from linear_attention_transformer->braindecode) (1.11.2)\n",
      "Requirement already satisfied: product-key-memory>=0.1.5 in /opt/micromamba/lib/python3.11/site-packages (from linear_attention_transformer->braindecode) (0.3.0)\n",
      "Requirement already satisfied: colt5-attention>=0.10.14 in /opt/micromamba/lib/python3.11/site-packages (from product-key-memory>=0.1.5->linear_attention_transformer->braindecode) (0.11.1)\n",
      "Requirement already satisfied: hyper-connections>=0.1.8 in /opt/micromamba/lib/python3.11/site-packages (from local-attention->linear_attention_transformer->braindecode) (0.4.7)\n",
      "Requirement already satisfied: aiohttp>=3.10.11 in /opt/micromamba/lib/python3.11/site-packages (from wfdb->braindecode) (3.12.15)\n",
      "Requirement already satisfied: pandas>=2.2.3 in /opt/micromamba/lib/python3.11/site-packages (from wfdb->braindecode) (2.3.2)\n",
      "Requirement already satisfied: soundfile>=0.10.0 in /opt/micromamba/lib/python3.11/site-packages (from wfdb->braindecode) (0.13.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp>=3.10.11->wfdb->braindecode) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp>=3.10.11->wfdb->braindecode) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp>=3.10.11->wfdb->braindecode) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp>=3.10.11->wfdb->braindecode) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp>=3.10.11->wfdb->braindecode) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp>=3.10.11->wfdb->braindecode) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp>=3.10.11->wfdb->braindecode) (1.20.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/micromamba/lib/python3.11/site-packages (from pandas>=2.2.3->wfdb->braindecode) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/micromamba/lib/python3.11/site-packages (from pandas>=2.2.3->wfdb->braindecode) (2025.2)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/micromamba/lib/python3.11/site-packages (from soundfile>=0.10.0->wfdb->braindecode) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /opt/micromamba/lib/python3.11/site-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb->braindecode) (2.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install braindecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1cbd1da8-37d7-4ad3-9ba4-9a09bf88504e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed final_layer.weight from state_dict to allow 2-class head.\n",
      "Removed final_layer.bias from state_dict to allow 2-class head.\n",
      "SUCCESS: Encoder weights loaded. Final layer remains initialized for 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from braindecode.models import Labram\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# 1. Initialize with the specs that matched (64 channels, 1600 samples)\n",
    "model = Labram(\n",
    "    n_chans=64, \n",
    "    n_times=1600, \n",
    "    n_outputs=2\n",
    ")\n",
    "\n",
    "# 2. Load the state dict\n",
    "url = \"https://huggingface.co/braindecode/Labram-Braindecode/resolve/main/braindecode_labram_base.pt\"\n",
    "state = torch.hub.load_state_dict_from_url(url, map_location='cpu')\n",
    "\n",
    "if 'model' in state:\n",
    "    state = state['model']\n",
    "\n",
    "# 3. MANUALLY REMOVE the mismatched head layers\n",
    "# This is the crucial step to prevent the RuntimeError\n",
    "keys_to_remove = [\"final_layer.weight\", \"final_layer.bias\"]\n",
    "for key in keys_to_remove:\n",
    "    if key in state:\n",
    "        del state[key]\n",
    "        print(f\"Removed {key} from state_dict to allow 2-class head.\")\n",
    "\n",
    "# 4. Now load with strict=False\n",
    "# This will load the encoder and leave your new final_layer randomized\n",
    "model.load_state_dict(state, strict=False)\n",
    "print(\"SUCCESS: Encoder weights loaded. Final layer remains initialized for 2 classes.\")\n",
    "\n",
    "import torch.nn as nn\n",
    "from argparse import Namespace\n",
    "\n",
    "class LabramTrainerWrapper(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.model = base_model\n",
    "        # Create a mock config to satisfy the Trainer's internal checks\n",
    "        self.config = Namespace(problem_type=\"single_label_classification\") \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, inputs, labels=None):\n",
    "        # inputs: (Batch, 64, 1600)\n",
    "        logits = self.model(inputs)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "            \n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "model_for_trainer = LabramTrainerWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ddbe4ec-cc6c-46aa-828b-b5468be2a6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, cohen_kappa_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    acc = balanced_accuracy_score(labels, predictions)\n",
    "    kappa = cohen_kappa_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        \"balanced_accuracy\": acc,\n",
    "        \"kappa\": kappa\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4b1d98d7-01cb-4229-9750-31be4308aeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./labram_finetuning\",\n",
    "    learning_rate=5e-6,           # Low learning rate for foundation models\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=20,          # Allow enough epochs for the head to adapt\n",
    "    weight_decay=0.1,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",  # Monitor cross-dataset performance every epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"balanced_accuracy\",\n",
    "    remove_unused_columns=False,  # DO NOT change this\n",
    "    report_to=\"none\" ,             # Change to \"wandb\" if you use it\n",
    "    label_smoothing_factor=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "21b1ebe6-4995-4709-a764-ea0306769be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total BCI Training Trials: 1152\n",
      "Total PhysioNet Test Trials: 144\n",
      "Input shape: torch.Size([64, 1600])\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total BCI Training Trials: {len(train_dataset)}\")\n",
    "print(f\"Total PhysioNet Test Trials: {len(validation_dataset)}\")\n",
    "\n",
    "# Check a single sample shape\n",
    "sample = train_dataset[0]\n",
    "print(f\"Input shape: {sample['inputs'].shape}\") # Should be [64, 1600]\n",
    "print(f\"Label: {sample['labels']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dfc348e3-3982-45f9-b408-770b66b909e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='998' max='1440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 998/1440 06:05 < 02:42, 2.72 it/s, Epoch 13.85/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>Kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.693700</td>\n",
       "      <td>0.693228</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.691400</td>\n",
       "      <td>0.692991</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.685100</td>\n",
       "      <td>0.693067</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.675700</td>\n",
       "      <td>0.696384</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>-0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.652300</td>\n",
       "      <td>0.696075</td>\n",
       "      <td>0.513889</td>\n",
       "      <td>0.027778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.637000</td>\n",
       "      <td>0.699555</td>\n",
       "      <td>0.493056</td>\n",
       "      <td>-0.013889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.605900</td>\n",
       "      <td>0.702777</td>\n",
       "      <td>0.493056</td>\n",
       "      <td>-0.013889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.588900</td>\n",
       "      <td>0.714710</td>\n",
       "      <td>0.493056</td>\n",
       "      <td>-0.013889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.574800</td>\n",
       "      <td>0.725582</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>-0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.525700</td>\n",
       "      <td>0.741276</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>-0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.524600</td>\n",
       "      <td>0.749401</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>-0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.499000</td>\n",
       "      <td>0.758633</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>-0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.472900</td>\n",
       "      <td>0.761120</td>\n",
       "      <td>0.493056</td>\n",
       "      <td>-0.013889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      2\u001b[39m trainer = Trainer(\n\u001b[32m      3\u001b[39m     model=model_for_trainer,\n\u001b[32m      4\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 1. Start Training (Fine-tuning)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# 2. Final Evaluation\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Final Cross-Dataset Results ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/transformers/trainer.py:2328\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/transformers/trainer.py:2677\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2671\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m   2672\u001b[39m     tr_loss_step = \u001b[38;5;28mself\u001b[39m.training_step(model, inputs, num_items_in_batch)\n\u001b[32m   2674\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2675\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2676\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m-> \u001b[39m\u001b[32m2677\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2678\u001b[39m ):\n\u001b[32m   2679\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2680\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n\u001b[32m   2681\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model_for_trainer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,      # BCI IV 2a\n",
    "    eval_dataset=validation_dataset,    # PhysioNet\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 1. Start Training (Fine-tuning)\n",
    "trainer.train()\n",
    "\n",
    "# 2. Final Evaluation\n",
    "print(\"\\n--- Final Cross-Dataset Results ---\")\n",
    "final_results = trainer.evaluate()\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8f27903-112c-407c-b571-12da7ba867ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score, \n",
    "    cohen_kappa_score, \n",
    "    f1_score, \n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# Get predictions on PhysioNet\n",
    "predictions = trainer.predict(test_dataset_EEG)\n",
    "y_pred = np.argmax(predictions.predictions, axis=-1)\n",
    "y_true = predictions.label_ids\n",
    "\n",
    "# Plot\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Left', 'Right'], yticklabels=['Left', 'Right'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Cross-Dataset Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf4cf32-fcf3-44a1-a746-650b508ff7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
