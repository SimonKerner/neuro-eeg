{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee6a7264",
   "metadata": {},
   "source": [
    "# EEGMMIDB — Data Exploration & NeuroGPT Classification\n",
    "\n",
    "Goal:\n",
    "- Load the EEGMMIDB motor movement / imagery dataset\n",
    "- Explore signals and labels\n",
    "- Classification with NeuroGPT usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb62aefd-3ec5-4a4d-93cd-37ca32a82f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/jovyan/neuro-eeg\n",
      "Data path: /home/jovyan/neuro-eeg/data/physionet.org/files/eegmmidb/1.0.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Project paths\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "SRC_PATH = PROJECT_ROOT / \"src\"\n",
    "DATA_PATH = PROJECT_ROOT / \"data\" / \"physionet.org\" / \"files\" / \"eegmmidb\" / \"1.0.0\"\n",
    "\n",
    "sys.path.append(str(SRC_PATH))\n",
    "\n",
    "from dataloader import EEGMMIDBDataset\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "print(\"Data path:\", DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c92f5272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Af3.', 'Af4.', 'Af7.', 'Af8.', 'Afz.', 'C1..', 'C2..', 'C3..', 'C4..', 'C5..', 'C6..', 'Cp1.', 'Cp2.', 'Cp3.', 'Cp4.', 'Cp5.', 'Cp6.', 'Cpz.', 'Cz..', 'F1..', 'F2..', 'F3..', 'F4..', 'F5..', 'F6..', 'F7..', 'F8..', 'Fc1.', 'Fc2.', 'Fc3.', 'Fc4.', 'Fc5.', 'Fc6.', 'Fcz.', 'Fp1.', 'Fp2.', 'Fpz.', 'Ft7.', 'Ft8.', 'Fz..', 'Iz..', 'O1..', 'O2..', 'Oz..', 'P1..', 'P2..', 'P3..', 'P4..', 'P5..', 'P6..', 'P7..', 'P8..', 'Po3.', 'Po4.', 'Po7.', 'Po8.', 'Poz.', 'Pz..', 'T10.', 'T7..', 'T8..', 'T9..', 'Tp7.', 'Tp8.']\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "\n",
    "edf = DATA_PATH / \"S001\" / \"S001R03.edf\"\n",
    "raw = mne.io.read_raw_edf(edf, preload=False, verbose=False)\n",
    "\n",
    "print(sorted(raw.ch_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f0c892",
   "metadata": {},
   "source": [
    "## Load Dataset for exploration\n",
    "\n",
    "Subject 1, motor execution + imagery, left/right hand\n",
    "\n",
    "Classes:\n",
    "- 0: Right imagined\n",
    "- 1: Right real\n",
    "- 2: Left imagined\n",
    "- 3: Left real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71a86bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total extracted trials: 90\n"
     ]
    }
   ],
   "source": [
    "dataset = EEGMMIDBDataset(\n",
    "    root_path=str(DATA_PATH),\n",
    "    subjects=[1],\n",
    "    runs=[3, 4, 7, 8, 11, 12],\n",
    "    t_min=0.0,\n",
    "    t_max=2.0,\n",
    "    normalization=True\n",
    ")\n",
    "\n",
    "print(\"Total extracted trials:\", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dfadd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset-level class counts:\n",
      "Right Imagined : 22\n",
      "Right Real     : 22\n",
      "Left Imagined  : 23\n",
      "Left Real      : 23\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "labels = [dataset[i][\"labels\"].item() for i in range(len(dataset))]\n",
    "\n",
    "label_map = {\n",
    "    0: \"Right Imagined\",\n",
    "    1: \"Right Real\",\n",
    "    2: \"Left Imagined\",\n",
    "    3: \"Left Real\"\n",
    "}\n",
    "\n",
    "counts = Counter(labels)\n",
    "\n",
    "print(\"Dataset-level class counts:\")\n",
    "for k in sorted(label_map):\n",
    "    print(f\"{label_map[k]:<15}: {counts.get(k, 0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "792e70e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw EDF annotation summary (Subject 1):\n",
      "\n",
      "Run 3: {'T0': 1, 'T1': 1, 'T2': 1}\n",
      "Run 4: {'T0': 1, 'T1': 1, 'T2': 1}\n",
      "Run 7: {'T0': 1, 'T1': 1, 'T2': 1}\n",
      "Run 8: {'T0': 1, 'T1': 1, 'T2': 1}\n",
      "Run 11: {'T0': 1, 'T1': 1, 'T2': 1}\n",
      "Run 12: {'T0': 1, 'T1': 1, 'T2': 1}\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "from collections import defaultdict\n",
    "\n",
    "event_summary = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for run in [3, 4, 7, 8, 11, 12]:\n",
    "    edf_path = DATA_PATH / \"S001\" / f\"S001R{run:02d}.edf\"\n",
    "    raw = mne.io.read_raw_edf(edf_path, preload=False, verbose=False)\n",
    "    \n",
    "    _, event_id = mne.events_from_annotations(raw, verbose=False)\n",
    "    \n",
    "    for label in event_id.keys():\n",
    "        event_code = label.split(\"/\")[-1]\n",
    "        event_summary[run][event_code] += 1\n",
    "\n",
    "print(\"Raw EDF annotation summary (Subject 1):\\n\")\n",
    "for run, events in event_summary.items():\n",
    "    print(f\"Run {run}: {dict(events)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd9439ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample-wise label sanity check (first 10 trials):\n",
      "\n",
      "Trial 00 -> Class 1: Right Real\n",
      "Trial 01 -> Class 3: Left Real\n",
      "Trial 02 -> Class 3: Left Real\n",
      "Trial 03 -> Class 1: Right Real\n",
      "Trial 04 -> Class 1: Right Real\n",
      "Trial 05 -> Class 3: Left Real\n",
      "Trial 06 -> Class 3: Left Real\n",
      "Trial 07 -> Class 1: Right Real\n",
      "Trial 08 -> Class 3: Left Real\n",
      "Trial 09 -> Class 1: Right Real\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample-wise label sanity check (first 10 trials):\\n\")\n",
    "\n",
    "for i in range(10):\n",
    "    sample = dataset[i]\n",
    "    label = sample[\"labels\"].item()\n",
    "    print(f\"Trial {i:02d} -> Class {label}: {label_map[label]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d02ad24",
   "metadata": {},
   "source": [
    "## Visual comparison: Right Imagined vs Right Real EEG signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c33acf68",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 7 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, ch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(channel_indices):\n\u001b[32m     59\u001b[39m     plt.subplot(\u001b[38;5;28mlen\u001b[39m(channel_indices), \u001b[32m1\u001b[39m, i + \u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     plt.plot(time, \u001b[43mimagined_avg\u001b[49m\u001b[43m[\u001b[49m\u001b[43mch_idx\u001b[49m\u001b[43m]\u001b[49m.numpy(), label=\u001b[33m\"\u001b[39m\u001b[33mRight Imagined\u001b[39m\u001b[33m\"\u001b[39m, alpha=\u001b[32m0.8\u001b[39m)\n\u001b[32m     61\u001b[39m     plt.plot(time, real_avg[ch_idx].numpy(), label=\u001b[33m\"\u001b[39m\u001b[33mRight Real\u001b[39m\u001b[33m\"\u001b[39m, alpha=\u001b[32m0.8\u001b[39m)\n\u001b[32m     62\u001b[39m     plt.ylabel(channel_names[i])\n",
      "\u001b[31mIndexError\u001b[39m: index 7 is out of bounds for dimension 0 with size 1"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "# sample channels\n",
    "channel_names = [\n",
    "    \"C3\", \"C4\", \"Cz\", \"F3\", \"F4\"\n",
    "]\n",
    "\n",
    "\n",
    "neurogpt_channels = [\n",
    "    \"Fp1\",\"Fp2\",\"F7\",\"F3\",\"Fz\",\"F4\",\"F8\",\n",
    "    \"T1\",\"T3\",\"C3\",\"Cz\",\"C4\",\"T4\",\"T2\",\n",
    "    \"T5\",\"P3\",\"Pz\",\"P4\",\"T6\",\"O1\",\"Oz\",\"O2\"\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# Focus on the Motor Strip (C-line) and surrounding areas\n",
    "channel_names = [\n",
    "    \"C3\",   # Primary Left Motor Cortex (Contralateral to Right Hand - MOST IMPORTANT)\n",
    "    \"Cz\",   # Midline Motor Cortex\n",
    "    \"C4\",   # Primary Right Motor Cortex (Ipsilateral)\n",
    "    \"FC3\",  # Pre-motor area (Left)\n",
    "    \"CP3\"   # Somatosensory/Post-central area (Left)\n",
    "]\n",
    "\n",
    "neurogpt_channels = [\n",
    "    \"Fz\", \"FC3\", \"FC1\", \"FCz\", \"FC2\", \"FC4\", \"C5\", \"C3\", \"C1\", \"Cz\", \n",
    "    \"C2\", \"C4\", \"C6\", \"CP3\", \"CP1\", \"CPz\", \"CP2\", \"CP4\", \"P1\", \"Pz\", \"P2\", \"POz\"\n",
    "]\n",
    "\n",
    "channel_indices = [neurogpt_channels.index(ch) for ch in channel_names]\n",
    "\n",
    "# Collect trials\n",
    "imagined_trials = []\n",
    "real_trials = []\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    sample = dataset[i]\n",
    "    label = sample[\"labels\"].item()\n",
    "\n",
    "    # Right Imagined = 0, Right Real = 1\n",
    "    if label == 0:\n",
    "        imagined_trials.append(sample[\"inputs\"])\n",
    "    elif label == 1:\n",
    "        real_trials.append(sample[\"inputs\"])\n",
    "\n",
    "# Compute average waveform per class\n",
    "imagined_avg = torch.stack(imagined_trials).mean(dim=0)\n",
    "real_avg = torch.stack(real_trials).mean(dim=0)\n",
    "\n",
    "# Time axis (seconds)\n",
    "sfreq = 250\n",
    "time = np.arange(imagined_avg.shape[1]) / sfreq\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, ch_idx in enumerate(channel_indices):\n",
    "    plt.subplot(len(channel_indices), 1, i + 1)\n",
    "    plt.plot(time, imagined_avg[ch_idx].numpy(), label=\"Right Imagined\", alpha=0.8)\n",
    "    plt.plot(time, real_avg[ch_idx].numpy(), label=\"Right Real\", alpha=0.8)\n",
    "    plt.ylabel(channel_names[i])\n",
    "    if i == 0:\n",
    "        plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.suptitle(\"Subject 1 — Right Hand: Imagined vs Real EEG (Average Waveforms)\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6678bb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean signal energy per class:\n",
      "Right Imagined : 0.9163\n",
      "Right Real     : 0.9236\n",
      "Left Imagined  : 0.9129\n",
      "Left Real      : 0.9253\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "energy_by_class = defaultdict(list)\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    sample = dataset[i]\n",
    "    label = sample[\"labels\"].item()\n",
    "    energy = sample[\"inputs\"].pow(2).mean().item()\n",
    "    energy_by_class[label].append(energy)\n",
    "\n",
    "print(\"Mean signal energy per class:\")\n",
    "for k in sorted(label_map):\n",
    "    print(f\"{label_map[k]:<15}: {np.mean(energy_by_class[k]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d88cd98",
   "metadata": {},
   "source": [
    "## NeuroGPT Teaser\n",
    "\n",
    "Each trial is already:\n",
    "- 22-channel (NeuroGPT format)\n",
    "- Normalized\n",
    "- Fixed-length\n",
    "- Labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93a9b164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 22, 500])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "batch = dataset[0]\n",
    "print(batch[\"inputs\"].shape)\n",
    "print(batch[\"labels\"].item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3daa241",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed89fab4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd0c63cc",
   "metadata": {},
   "source": [
    "## Loading the train, test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8386c110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjects: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108]\n",
      "MI_ME runs: [3, 4, 7, 8, 11, 12]\n"
     ]
    }
   ],
   "source": [
    "all_subjects = list(range(1, 109))  # subject range\n",
    "print(\"Subjects:\", all_subjects)\n",
    "\n",
    "MI_ME_RUNS = [3, 4, 7, 8, 11, 12]\n",
    "print(\"MI_ME runs:\", MI_ME_RUNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2db9cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folds\n",
    "\n",
    "train_folds = []\n",
    "test_folds = []\n",
    "\n",
    "for i in range(len(all_subjects) // 2):\n",
    "    test_subjects = all_subjects[i*2 : i*2+2]\n",
    "    train_subjects = all_subjects[:i*2] + all_subjects[i*2+2:]\n",
    "\n",
    "    train_folds.append(train_subjects)\n",
    "    test_folds.append(test_subjects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9fb3485-15d1-4ac6-a339-e28fd8ef63a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/neuro-eeg/src/dataloader.py:143: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw = mne.io.read_raw_edf(edf_path, preload=True, verbose=False)\n",
      "/home/jovyan/neuro-eeg/src/dataloader.py:143: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw = mne.io.read_raw_edf(edf_path, preload=True, verbose=False)\n",
      "/home/jovyan/neuro-eeg/src/dataloader.py:143: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw = mne.io.read_raw_edf(edf_path, preload=True, verbose=False)\n",
      "/home/jovyan/neuro-eeg/src/dataloader.py:143: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw = mne.io.read_raw_edf(edf_path, preload=True, verbose=False)\n",
      "/home/jovyan/neuro-eeg/src/dataloader.py:143: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw = mne.io.read_raw_edf(edf_path, preload=True, verbose=False)\n",
      "/home/jovyan/neuro-eeg/src/dataloader.py:143: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw = mne.io.read_raw_edf(edf_path, preload=True, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "from src.dataloader import EEGMMIDBDataset\n",
    "\n",
    "train_dataset = EEGMMIDBDataset(\n",
    "    root_path=str(DATA_PATH),\n",
    "    subjects=train_subjects,\n",
    "    runs=MI_ME_RUNS,\n",
    "    t_min=0.0,\n",
    "    t_max=2.0,\n",
    "    normalization=True\n",
    ")\n",
    "\n",
    "test_dataset = EEGMMIDBDataset(\n",
    "    root_path=str(DATA_PATH),\n",
    "    subjects=test_subjects,\n",
    "    runs=MI_ME_RUNS,\n",
    "    t_min=0.0,\n",
    "    t_max=2.0,\n",
    "    normalization=True\n",
    ")\n",
    "\n",
    "# Simple setup (same as your example)\n",
    "validation_dataset = test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "907856ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train trials: 9575\n",
      "Test trials : 180\n",
      "Input shape: torch.Size([1, 22, 500])\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Train trials:\", len(train_dataset))\n",
    "print(\"Test trials :\", len(test_dataset))\n",
    "\n",
    "batch = train_dataset[0]\n",
    "print(\"Input shape:\", batch[\"inputs\"].shape)   # (22, T)\n",
    "print(\"Label:\", batch[\"labels\"].item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267da58c-ef7a-4478-9083-617fd2014b99",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c947d26c-3d05-40a2-9fa7-f59ad6b6cb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "home = os.environ[\"HOME\"]\n",
    "python_imports = f\"{home}/shared/DLN-2025W/notebook-eeg/Lab_GPT_based_EEG_decoder\"\n",
    "cache_root = f\"{home}/shared/DLN-2025W/notebook-eeg/Lab_GPT_based_EEG_decoder/\"\n",
    "sys.path.append(python_imports)\n",
    "\n",
    "with open(os.path.join(\"NeuroGPT_mini/config.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4df973ee-057b-48da-b817-248b1a0ae973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import sys \n",
    "sys.path.insert(0,os.path.join('NeuroGPT_mini/') )\n",
    "from sklearn.metrics import balanced_accuracy_score, cohen_kappa_score, f1_score\n",
    "import random\n",
    "import json\n",
    "\n",
    "### Import related to Transformer model (from files located in /NeuroGPT directory)\n",
    "\n",
    "from encoder.conformer_braindecode import EEGConformer\n",
    "from decoder.make_decoder import make_decoder\n",
    "from embedder.make import make_embedder\n",
    "from trainer.make import make_trainer\n",
    "from trainer.base import Trainer\n",
    "from decoder.unembedder import make_unembedder\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a848704-1022-4e0b-83b5-f781d34abf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloade.MIdataset import MotorImageryDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb045469-882c-4c44-9eed-1973a25d2f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = {\"dst_data_path\" : os.path.join(python_imports, \"bciiv2a_eeg_npz/\"),\n",
    "         \"pretrained_model\" : os.path.join(\"NeuroGPT_mini/pytorch_model.bin\"), \n",
    "         \"log_dir\" :os.path.join(python_imports,\"training_logs/\")}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a8b72ff-6306-47cf-a75c-8353ace22e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files from all the subjects :  ['A01E.npz', 'A01T.npz', 'A02E.npz', 'A02T.npz', 'A03E.npz', 'A03T.npz', 'A04E.npz', 'A04T.npz', 'A05E.npz', 'A05T.npz', 'A06E.npz', 'A06T.npz', 'A07E.npz', 'A07T.npz', 'A08E.npz', 'A08T.npz', 'A09E.npz', 'A09T.npz']\n",
      "Train dataset : \n",
      "['A01E.npz', 'A01T.npz', 'A02E.npz', 'A02T.npz', 'A03E.npz', 'A03T.npz', 'A04E.npz', 'A04T.npz', 'A05E.npz', 'A05T.npz', 'A06E.npz', 'A06T.npz', 'A07E.npz', 'A07T.npz', 'A08E.npz', 'A08T.npz']\n",
      "Number of subjects loaded:  16\n",
      "Test dataset :\n",
      "['A09E.npz', 'A09T.npz']\n",
      "Number of subjects loaded:  2\n"
     ]
    }
   ],
   "source": [
    "# Compute the dataset (train, test, validation) \n",
    "downstream_path = config_path[\"dst_data_path\"]\n",
    "filenames = sorted(os.listdir(downstream_path))[:18]\n",
    "print(\"Files from all the subjects : \", filenames)\n",
    "train_folds = []\n",
    "test_folds = []\n",
    "\n",
    "for i in range(9):\n",
    "    train_files = filenames[0:i*2] + filenames[i*2+2:]\n",
    "    test_files = filenames[i*2 : i*2+2]\n",
    "\n",
    "\n",
    "print(\"Train dataset : \")\n",
    "print(train_files)\n",
    "\n",
    "\n",
    "train_dataset = MotorImageryDataset(train_files, root_path=downstream_path)\n",
    "\n",
    "\n",
    "print(\"Test dataset :\")\n",
    "print(test_files)\n",
    "\n",
    "# On the way, compute the test dataset as :\n",
    "\n",
    "## TO DO : instanciate a MotorImageryDataset object but with the testing files\n",
    "# test_dataset = ... \n",
    "# YOUR CODE HERE\n",
    "validation_dataset = MotorImageryDataset(test_files, root_path=downstream_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7eccc5d8-b47f-4bf4-b2c3-db7f885ef11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Create Model object from embedder, decoder,\n",
    "    and unembedder (if not None).\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    embedder: src.embedder.make_embedder\n",
    "        Instance of embedder class.\n",
    "    decoder: src.decoder.make_decoder\n",
    "        Instance of decoder class.\n",
    "    unembedder: src.unembedder.make_unembedder\n",
    "        Instance of unembedder class.\n",
    "        Only added to model if not None.\n",
    "\n",
    "    Methods\n",
    "    ----\n",
    "    forward(batch: Dict[str, torch.tensor])\n",
    "        Forward pass of model.\n",
    "    prep_batch(batch: Dict[str, torch.tensor])\n",
    "        Prepare batch for forward pass.\n",
    "    compute_loss(batch: Dict[str, torch.tensor])\n",
    "        Compute training loss.\n",
    "    from_pretrained(pretrained_path: str)\n",
    "        Load pretrained model from pretrained_path.\n",
    "        Needs to point to pytorch_model.bin file \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: torch.nn.Module,\n",
    "        embedder: torch.nn.Module,\n",
    "        decoder: torch.nn.Module,\n",
    "        unembedder: torch.nn.Module = None\n",
    "        ) -> torch.nn.Module:\n",
    "        \n",
    "        super().__init__()\n",
    "        self.name = f'Embedder-{embedder.name}_Decoder-{decoder.name}'\n",
    "        self.encoder = encoder\n",
    "        self.embedder = embedder\n",
    "        self.decoder = decoder\n",
    "        self.unembedder = unembedder\n",
    "        self.is_decoding_mode = False\n",
    "        self.ft_only_encoder = False\n",
    "\n",
    "    def from_pretrained(\n",
    "        self,\n",
    "        pretrained_path: str\n",
    "        ) -> None:\n",
    "        \"\"\"Load pretrained model from pretrained_path.\n",
    "        Needs to point to pytorch_model.bin file.\n",
    "        \"\"\"\n",
    "        print(\n",
    "            f'Loading pretrained model from {pretrained_path}'\n",
    "        )\n",
    "\n",
    "        if next(self.parameters()).is_cuda:\n",
    "            pretrained = torch.load(pretrained_path)\n",
    "\n",
    "        else:\n",
    "            pretrained = torch.load(pretrained_path, map_location=torch.device('cpu'))\n",
    "        \n",
    "        for k in self.state_dict():\n",
    "            \n",
    "            if k in pretrained:\n",
    "                assert pretrained[k].shape == self.state_dict()[k].shape,\\\n",
    "                    f'{k} shape mismatch between pretrained model and current model '+\\\n",
    "                    f'{pretrained[k].shape} vs {self.state_dict()[k].shape}'\n",
    "        \n",
    "        for k in pretrained:     \n",
    "            if k not in self.state_dict():\n",
    "                warnings.warn(\n",
    "                    f'Warning: /!\\ Skipping {k} from {pretrained_path} '\\\n",
    "                    'because it is not part of the current model'\n",
    "                )\n",
    "\n",
    "        # we set strict=False, because we can be sure\n",
    "        # that all relevant keys are in pretrained\n",
    "        self.load_state_dict(pretrained, strict=False)\n",
    "        \n",
    "    def switch_ft_mode(self, ft_encoder_only=False):\n",
    "        self.ft_only_encoder = ft_encoder_only\n",
    "\n",
    "    def switch_decoding_mode(\n",
    "        self,\n",
    "        is_decoding_mode: bool = False,\n",
    "        num_decoding_classes: int = None\n",
    "        ) -> None:\n",
    "        \"\"\"Switch model to decoding model or back to training mode.\n",
    "        Necessary to adapt pre-trained models to downstream\n",
    "        decoding tasks.\n",
    "        \n",
    "        Args\n",
    "        ----\n",
    "        is_decoding_mode: bool\n",
    "            Whether to switch to decoding mode or not.\n",
    "        num_decoding_classes: int\n",
    "            Number of classes to use for decoding.    \n",
    "        \"\"\"\n",
    "        self.is_decoding_mode = is_decoding_mode\n",
    "        \n",
    "        self.embedder.switch_decoding_mode(is_decoding_mode=is_decoding_mode)\n",
    "        self.decoder.switch_decoding_mode(\n",
    "            is_decoding_mode=is_decoding_mode,\n",
    "            num_decoding_classes=num_decoding_classes\n",
    "        )\n",
    "\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        batch: Dict[str, torch.tensor],\n",
    "        return_outputs: bool = False\n",
    "        ) -> Dict[str, torch.tensor]:\n",
    "        \"\"\"\n",
    "        Compute training loss, based on \n",
    "        embedder's training-style.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        batch: Dict[str, torch.tensor]\n",
    "            Input batch (as generated by src.batcher)\n",
    "        return_outputs: bool\n",
    "            Whether to return outputs of forward pass\n",
    "            or not. If False, only loss is returned.\n",
    "\n",
    "        Returns\n",
    "        ----\n",
    "        losses: Dict[str, torch.tensor]\n",
    "            Training losses.\n",
    "        outputs: torch.tensor\n",
    "            Outputs of forward pass.\n",
    "        \"\"\"\n",
    "        (outputs, batch) = self.forward(\n",
    "            batch=batch,\n",
    "            return_batch=True\n",
    "        )\n",
    "        losses = self.embedder.loss(\n",
    "            batch=batch,\n",
    "            outputs=outputs\n",
    "        )\n",
    "\n",
    "        return (losses, outputs) if return_outputs else losses\n",
    "\n",
    "    def prep_batch(\n",
    "        self,\n",
    "        batch: Dict[str, torch.tensor]\n",
    "        ) -> Dict[str, torch.tensor]:\n",
    "        \"\"\"Prepare input batch for forward pass.\n",
    "        Calls src.embedder.prep_batch.\n",
    "        \n",
    "        Args\n",
    "        ----\n",
    "        batch: Dict[str, torch.tensor]\n",
    "            Input batch (as generated by src.batcher)\n",
    "        \"\"\"\n",
    "        return self.embedder.prep_batch(batch=dict(batch))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        batch: Dict[str, torch.tensor],\n",
    "        prep_batch: bool = True,\n",
    "        return_batch: bool = False\n",
    "        ) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of model.\n",
    "        \n",
    "        Args\n",
    "        ----\n",
    "        batch: Dict[str, torch.tensor]\n",
    "            Input batch (as generated by src.batcher)\n",
    "        prep_batch: bool\n",
    "            Whether to prep batch for forward pass\n",
    "            by calling self.embedder.prep_batch\n",
    "        return_batch: bool\n",
    "            Whether to return batch after forward pass\n",
    "            or not. If False, only outputs of forward pass\n",
    "            are returned.\n",
    "\n",
    "        Returns\n",
    "        ----\n",
    "        outputs: torch.tensor\n",
    "            Outputs of forward pass.\n",
    "        batch: Dict[str, torch.tensor]\n",
    "            Input batch (as returned by prep_batch, \n",
    "            if prep_batch is True)\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.encoder is not None:\n",
    "            #before prep_batch masking and things, we need to first let the splitted chunks of raw input through the encoder\n",
    "            features = self.encoder(batch['inputs'])\n",
    "            #attempt for trying fine-tune only the encoder, but the encoder cannot combine information across chunks.\n",
    "            if self.is_decoding_mode and self.ft_only_encoder:\n",
    "                outputs={'outputs': features, 'decoding_logits': features}\n",
    "                return (outputs, batch) if return_batch else outputs\n",
    "\n",
    "            b, f1, f2 = features.size()\n",
    "            nchunks = batch['inputs'].size()[1]\n",
    "            batch['inputs'] = features.view(b//nchunks, nchunks, f1*f2)\n",
    "        \n",
    "        if prep_batch:\n",
    "            if len(batch['inputs'].size()) > 3:\n",
    "                bsize, chunk, chann, time = batch['inputs'].size() \n",
    "                batch['inputs'] = batch['inputs'].view(bsize, chunk, chann*time)\n",
    "            batch = self.prep_batch(batch=batch)\n",
    "            # batch['inputs_embeds'] = batch['inputs_embeds'].view(bsize, chunk, chann, time)\n",
    "            # print(\"preparing batch\")\n",
    "        else:\n",
    "            assert 'inputs_embeds' in batch, 'inputs_embeds not in batch'\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        batch['inputs_embeds'] = self.embedder(batch=batch)\n",
    "        outputs = self.decoder(batch=batch)\n",
    "        \n",
    "        if self.unembedder is not None and not self.is_decoding_mode:\n",
    "            outputs['outputs'] = self.unembedder(inputs=outputs['outputs'])['outputs']\n",
    "\n",
    "        return (outputs, batch) if return_batch else outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f95ca59f-4ef1-4d67-a0ee-a965fd0cda06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(model_config) : \n",
    "# Generate the model\n",
    "    \n",
    "    \n",
    "    ## Encoder\n",
    "    \n",
    "    if model_config[\"use_encoder\"] == True:\n",
    "        \n",
    "        chann_coords = None\n",
    "        encoder = EEGConformer(n_outputs=model_config[\"num_decoding_classes\"], n_chans=22, n_times=model_config['chunk_len'], ch_pos=chann_coords, is_decoding_mode=model_config[\"ft_only_encoder\"])\n",
    "        #calculates the output dimension of the encoder, which is the output of transformer layer.\n",
    "        model_config[\"parcellation_dim\"] = ((model_config['chunk_len'] - model_config['filter_time_length'] + 1 - model_config['pool_time_length']) // model_config['stride_avg_pool'] + 1) * model_config['n_filters_time']\n",
    "\n",
    "    else:\n",
    "        encoder = None\n",
    "        model_config[\"parcellation_dim\"] = model_config[\"chunk_len\"] * 22\n",
    "    \n",
    "    ## Embedder\n",
    "    \n",
    "    embedder = make_embedder(\n",
    "        training_style=model_config[\"training_style\"],\n",
    "        architecture=model_config[\"architecture\"],\n",
    "        in_dim=model_config[\"parcellation_dim\"], # flattened, channel x chunk length\n",
    "        embed_dim=model_config[\"embedding_dim\"],\n",
    "        num_hidden_layers=model_config[\"num_hidden_layers_embedding_model\"],\n",
    "        dropout=model_config[\"dropout\"],\n",
    "        n_positions=model_config[\"n_positions\"]\n",
    "    )\n",
    "    \n",
    "    ## Decoder\n",
    "    decoder = make_decoder(\n",
    "        architecture=model_config[\"architecture\"],\n",
    "        num_hidden_layers=model_config[\"num_hidden_layers\"],\n",
    "        embed_dim=model_config[\"embedding_dim\"],\n",
    "        num_attention_heads=model_config[\"num_attention_heads\"],\n",
    "        n_positions=model_config[\"n_positions\"],\n",
    "        intermediate_dim_factor=model_config[\"intermediate_dim_factor\"],\n",
    "        hidden_activation=model_config[\"hidden_activation\"],\n",
    "        dropout=model_config[\"dropout\"]\n",
    "    )\n",
    "   \n",
    "    \n",
    "    if model_config[\"embedding_dim\"] != model_config[\"parcellation_dim\"]:\n",
    "        unembedder = make_unembedder(\n",
    "            embed_dim=model_config[\"embedding_dim\"],\n",
    "            num_hidden_layers=model_config[\"num_hidden_layers_unembedding_model\"],\n",
    "            out_dim=model_config[\"parcellation_dim\"],\n",
    "            dropout=model_config[\"dropout\"],\n",
    "        )\n",
    "    else:\n",
    "        print(\"No Embedder and Unembedder!\")\n",
    "        unembedder = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = Model(\n",
    "        encoder=encoder,\n",
    "        embedder=embedder,\n",
    "        decoder=decoder,\n",
    "        unembedder=unembedder\n",
    "    )\n",
    "    \n",
    "    if model_config[\"ft_only_encoder\"]:\n",
    "        model.switch_ft_mode(ft_encoder_only=True)\n",
    "    \n",
    "    if model_config[\"training_style\"] == 'decoding':\n",
    "        model.switch_decoding_mode(\n",
    "            is_decoding_mode=True,\n",
    "            num_decoding_classes=model_config[\"num_decoding_classes\"]\n",
    "        )\n",
    "    \n",
    "    if model_config[\"pretrained_model\"] is not None:\n",
    "        model.from_pretrained(model_config[\"pretrained_model\"])\n",
    "    \n",
    "    if model_config[\"freeze_embedder\"]:\n",
    "        for param in model.embedder.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    if model_config[\"freeze_decoder\"]:\n",
    "\n",
    "        ## TO DO : freeze the parameters of the decoder module :\n",
    "        for param in model.decoder.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    if model_config[\"freeze_encoder\"]:\n",
    "        for name, param in model.encoder.named_parameters():\n",
    "            if 'fc.' in name \\\n",
    "            or 'final_layer' in name:\n",
    "                continue\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "        print('Frozen Encoder : Only the two last layers will be trained')\n",
    "    \n",
    "    if 'freeze_decoder_without_pooler_heads' in model_config \\\n",
    "        and model_config[\"freeze_decoder_without_pooler_heads\"]:\n",
    "        for name, param in model.decoder.named_parameters():\n",
    "            if 'pooler_layer' in name \\\n",
    "            or 'decoding_head' in name \\\n",
    "            or 'is_next_head' in name:\n",
    "    \n",
    "\n",
    "                continue\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    if model_config[\"freeze_unembedder\"] and unembedder is not None:\n",
    "        for param in model.unembedder.parameters():\n",
    "            param.requires_grad = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69ef0c94-d3b8-442d-b51c-43499d3374a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FC Layer for Classification created.\n",
      "Loading pretrained model from NeuroGPT_mini/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model_config = config\n",
    "\n",
    "## Some important parameters in the architecture of the model \n",
    "\n",
    "model_config['pretrained_model'] = config_path['pretrained_model'] # Path to the file containing pretrained weights of the model, if model_config['pretrained_model'] = None the model is not pretrained\n",
    "model_config['embedding_dim'] = 1024 # Dimension of the latent representations in the model\n",
    "model_config['num_hidden_layers_embedding_model']= 1 # Number of hidden layers in the GPT model \n",
    "model_config['num_hidden_layers_unembedding_model']= 1 # Number of hidden layers on the unembedding module \n",
    "model_config['num_hidden_layers']= 6  #Number of hidden layers in the encoder module\n",
    "model_config['filter_time_length']= 25 #Size of the kernel of the temporal convolution layer \n",
    "model_config['stride_avg_pool']= 15  # Stride size used in the average-pooling operation\n",
    "model_config[\"freeze_encoder\"] = False # Whether to freeze the encoder (True = no training on encoder parameters, only the classification layer)\n",
    "\n",
    "\n",
    "model = make_model(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3dde66a-9c48-418e-a02e-aac3b9628a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = config\n",
    "\n",
    "# Some important paramerter to build up the training stategy\n",
    "\n",
    "trainer_config['model_init'] = make_model # Function used to instanciate a model\n",
    "trainer_config['run_name'] = 'Finetunning_1' # Name of the run to save logs in a directory \n",
    "trainer_config['train_dataset'] = train_dataset \n",
    "trainer_config['validation_dataset'] = validation_dataset\n",
    "trainer_config['output_dir'] = os.path.join(config_path['log_dir'], trainer_config['run_name']) # Where to save training logs\n",
    "\n",
    "trainer_config['training_steps'] = 3000 # Number of training steps\n",
    "trainer_config['validation_steps'] = 500 # Number of validation steps\n",
    "# Whether to freeze the encoder (True = no training on encoder parameters)\n",
    "trainer_config['model_save_steps'] = config[\"training_steps\"]*2\n",
    "trainer_config['log_every_n_steps'] = 1000\n",
    "trainer_config['eval_every_n_steps'] = 500\n",
    "trainer_config['warmup_ratio'] = 0.01\n",
    "trainer_config['optim'] = \"adamw_torch\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d32eed5-024f-42a0-86d7-650879f8cecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FC Layer for Classification created.\n",
      "Loading pretrained model from NeuroGPT_mini/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer = make_trainer(model_init=lambda: make_model(model_config), config = trainer_config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b852a3c-ba46-4a31-8b6b-1ef86f99d8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FC Layer for Classification created.\n",
      "Loading pretrained model from NeuroGPT_mini/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 01:45, Epoch 20/21]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.443000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.088600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.843700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.746600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of the training ! \n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "print(\"End of the training ! \")\n",
    "save_path = os.path.join(config[\"log_dir\"], trainer_config['run_name'], 'model_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "44f13504-c120-422a-ac44-300eae1bda0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has been saved to :  training_logs/Finetunning_1/model_final\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a723de41-2592-4686-b460-9163b5466e23",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fcf669eb-e7a1-4875-999e-5f5b3cd3656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_from_trainer(trainer, test_dataset):\n",
    "    \"\"\"\n",
    "    This function takes as input a trainer that has already been trained \n",
    "    (So the the trainer.train() method must already been called on it)\n",
    "    And it returns the a dictionnary with the different performance metrics. \n",
    "    \"\"\"\n",
    "    test_prediction_ = trainer.predict(test_dataset)\n",
    "    test_preds = test_prediction_.predictions\n",
    "    test_labels = test_prediction_.label_ids\n",
    "    pred_label=np.argmax(test_preds, axis=1)\n",
    "\n",
    "    true_label = test_labels\n",
    "    pred_label = pred_label \n",
    "    balanced_acc = balanced_accuracy_score(true_label, pred_label)\n",
    "\n",
    "    kappa = cohen_kappa_score(true_label, pred_label)\n",
    "\n",
    "    weighted_f1 = f1_score(true_label, pred_label, average='weighted')\n",
    "\n",
    "    cm = confusion_matrix(true_label, pred_label)\n",
    "    \n",
    "    return {\n",
    "        'Balanced Accuracy': balanced_acc,\n",
    "        'Cohen s Kappa': kappa, \n",
    "        'Weighted F1-score': weighted_f1,\n",
    "        'Confusion Matrix': cm \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6790c046-f00a-45ac-b4e4-7052998d13d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: torch.Size([16, 1, 22, 500])\n"
     ]
    }
   ],
   "source": [
    "from src.dataloader import EEGMMIDBDataset\n",
    "sample_batch = next(iter(trainer.get_test_dataloader(test_dataset)))\n",
    "print(f\"Inputs shape: {sample_batch['inputs'].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "507fddca-b99d-44d8-be18-2c79fbf95032",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (8) to match target batch_size (16).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mget_performance_from_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mget_performance_from_trainer\u001b[39m\u001b[34m(trainer, test_dataset)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_performance_from_trainer\u001b[39m(trainer, test_dataset):\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m    This function takes as input a trainer that has already been trained \u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m    (So the the trainer.train() method must already been called on it)\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m    And it returns the a dictionnary with the different performance metrics. \u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     test_prediction_ = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     test_preds = test_prediction_.predictions\n\u001b[32m      9\u001b[39m     test_labels = test_prediction_.label_ids\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/transformers/trainer.py:3105\u001b[39m, in \u001b[36mTrainer.predict\u001b[39m\u001b[34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   3102\u001b[39m start_time = time.time()\n\u001b[32m   3104\u001b[39m eval_loop = \u001b[38;5;28mself\u001b[39m.prediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.use_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluation_loop\n\u001b[32m-> \u001b[39m\u001b[32m3105\u001b[39m output = \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPrediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\n\u001b[32m   3107\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3108\u001b[39m total_batch_size = \u001b[38;5;28mself\u001b[39m.args.eval_batch_size * \u001b[38;5;28mself\u001b[39m.args.world_size\n\u001b[32m   3109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_jit_compilation_time\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output.metrics:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/transformers/trainer.py:3210\u001b[39m, in \u001b[36mTrainer.evaluation_loop\u001b[39m\u001b[34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   3207\u001b[39m         batch_size = observed_batch_size\n\u001b[32m   3209\u001b[39m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3210\u001b[39m loss, logits, labels = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3211\u001b[39m inputs_decode = \u001b[38;5;28mself\u001b[39m._prepare_input(inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m args.include_inputs_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3213\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/neuro-eeg/NeuroGPT_mini/trainer/base.py:629\u001b[39m, in \u001b[36mTrainer.prediction_step\u001b[39m\u001b[34m(self, model, batch, prediction_loss_only, ignore_keys)\u001b[39m\n\u001b[32m    626\u001b[39m batch = \u001b[38;5;28mself\u001b[39m._move_batch_to_device(batch=batch)\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m     (loss, outputs) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    633\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m prediction_loss_only \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[32m    636\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (loss, outputs[\u001b[33m'\u001b[39m\u001b[33mdecoding_logits\u001b[39m\u001b[33m'\u001b[39m], batch[\u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/neuro-eeg/NeuroGPT_mini/trainer/base.py:663\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, batch, return_outputs, **kwargs)\u001b[39m\n\u001b[32m    657\u001b[39m     (losses, outputs) = model.module.compute_loss(\n\u001b[32m    658\u001b[39m         batch=batch,\n\u001b[32m    659\u001b[39m         return_outputs=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    660\u001b[39m     )\n\u001b[32m    662\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m     (losses, outputs) = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    666\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    668\u001b[39m loss = losses[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m losses.keys() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28msum\u001b[39m(losses.values())\n\u001b[32m    670\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (loss, outputs) \u001b[38;5;28;01mif\u001b[39;00m return_outputs \u001b[38;5;28;01melse\u001b[39;00m loss\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 135\u001b[39m, in \u001b[36mModel.compute_loss\u001b[39m\u001b[34m(self, batch, return_outputs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03mCompute training loss, based on \u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[33;03membedder's training-style.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    129\u001b[39m \u001b[33;03m    Outputs of forward pass.\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m (outputs, batch) = \u001b[38;5;28mself\u001b[39m.forward(\n\u001b[32m    132\u001b[39m     batch=batch,\n\u001b[32m    133\u001b[39m     return_batch=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    134\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m losses = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutputs\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (losses, outputs) \u001b[38;5;28;01mif\u001b[39;00m return_outputs \u001b[38;5;28;01melse\u001b[39;00m losses\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/neuro-eeg/NeuroGPT_mini/embedder/base.py:244\u001b[39m, in \u001b[36mBaseEmbedder.loss\u001b[39m\u001b[34m(self, batch, outputs)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloss\u001b[39m(\n\u001b[32m    238\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    239\u001b[39m     batch,\n\u001b[32m    240\u001b[39m     outputs\n\u001b[32m    241\u001b[39m     ) -> Dict[\u001b[38;5;28mstr\u001b[39m, torch.tensor]:\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_decoding_mode:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         losses = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoding_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moutputs\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    250\u001b[39m         losses = \u001b[38;5;28mself\u001b[39m._root_loss(\n\u001b[32m    251\u001b[39m             **batch,\n\u001b[32m    252\u001b[39m             **outputs\n\u001b[32m    253\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/neuro-eeg/NeuroGPT_mini/embedder/base.py:178\u001b[39m, in \u001b[36mBaseEmbedder.decoding_loss\u001b[39m\u001b[34m(self, decoding_logits, labels, **kwargs)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecoding_loss\u001b[39m(\n\u001b[32m    171\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    172\u001b[39m     decoding_logits,\n\u001b[32m   (...)\u001b[39m\u001b[32m    175\u001b[39m     ) -> Dict[\u001b[38;5;28mstr\u001b[39m, torch.tensor]:\n\u001b[32m    176\u001b[39m     \u001b[38;5;66;03m# pdb.set_trace()\u001b[39;00m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mdecoding_loss\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mxe_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mdecoding_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/torch/nn/modules/loss.py:1310\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1309\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m-> \u001b[39m\u001b[32m1310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1311\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1317\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/torch/nn/functional.py:3462\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3461\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3462\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3463\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3466\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3469\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: Expected input batch_size (8) to match target batch_size (16)."
     ]
    }
   ],
   "source": [
    "print(get_performance_from_trainer(trainer, test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea361a6-539b-4584-8f88-9e29655b7723",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Neuro EEG",
   "language": "python",
   "name": "neuro-eeg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
