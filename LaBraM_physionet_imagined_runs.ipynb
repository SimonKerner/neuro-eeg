{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "534c8b10-2510-400a-910f-2e87fccebfe9",
   "metadata": {},
   "source": [
    "# LaBraM — PhysioNet EEGMMIDB fine-tuning (ALL runs → Left vs Right)\n",
    "\n",
    "Goal: fine-tune a pretrained LaBraM checkpoint on PhysioNet EEGMMIDB using **just real runs**  \n",
    "(runs 3, 7, 11 = real), predict **Left vs Right**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50b2194f-f4b7-4a95-b256-529ecbc1afcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /home/jovyan/projects/neuro-eeg\n",
      "LABRAM_ROOT: /home/jovyan/projects/neuro-eeg/LaBraM exists? True\n",
      "Device: cuda\n",
      "torch: 2.8.0+cu128\n"
     ]
    }
   ],
   "source": [
    "import os, sys, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "LABRAM_ROOT = PROJECT_ROOT / \"LaBraM\"\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"LABRAM_ROOT:\", LABRAM_ROOT, \"exists?\", LABRAM_ROOT.exists())\n",
    "\n",
    "# Make LaBraM importable\n",
    "if str(LABRAM_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(LABRAM_ROOT))\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "print(\"torch:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f2df50-c87f-46ee-bb23-1e8f9d210705",
   "metadata": {},
   "source": [
    "## Data location\n",
    "\n",
    "We use the PhysioNet EEGMMIDB dataset from the repo's `data/` folder.\n",
    "\n",
    "Expected root:\n",
    "`data/physionet.org/files/eegmmidb/1.0.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e39ed72-6b46-4b13-8a10-febd151cfd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_ROOT: /home/jovyan/projects/neuro-eeg/data/physionet.org/files/eegmmidb/1.0.0\n",
      "Exists? True\n",
      "Num subject folders found: 109\n",
      "First 5 subject folders: ['S001', 'S002', 'S003', 'S004', 'S005']\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = PROJECT_ROOT / \"data\" / \"physionet.org\" / \"files\" / \"eegmmidb\" / \"1.0.0\"\n",
    "print(\"DATA_ROOT:\", DATA_ROOT)\n",
    "print(\"Exists?\", DATA_ROOT.exists())\n",
    "\n",
    "# Optional: list first few subject dirs\n",
    "if DATA_ROOT.exists():\n",
    "    subdirs = sorted([p.name for p in DATA_ROOT.iterdir() if p.is_dir() and p.name.startswith(\"S\")])\n",
    "    print(\"Num subject folders found:\", len(subdirs))\n",
    "    print(\"First 5 subject folders:\", subdirs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1326f8b3-b535-49c4-8e6e-403824bd4a1e",
   "metadata": {},
   "source": [
    "## Dataloader for  runs (3,7,11) → Left vs Right\n",
    "\n",
    "The dataloader:\n",
    "- reads EDF with MNE\n",
    "- selects the BCI-style 22 channels\n",
    "- preprocesses + resamples to 200 Hz\n",
    "- extracts 4-second segments\n",
    "- returns tensors shaped **(22, 4, 200)** for LaBraM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff04ee35-16e3-4987-8ad1-4fc7b20df7d7",
   "metadata": {},
   "source": [
    "## Imports: LaBraM model registration + new ALL-runs dataloader\n",
    "\n",
    "- `modeling_finetune` registers the LaBraM models into `timm`\n",
    "- `EEGMMIDBLaBraMAllRunsLRDataset` loads PhysioNet EEGMMIDB runs 3,4,7,8 and labels only Left/Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c7e6ab-2bd1-4d1c-9d25-20ef1922b986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaBraM model registered? True\n",
      "Num timm models: 615\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "# Important: this import registers the model names into timm\n",
    "import modeling_finetune  # from LaBraM/\n",
    "\n",
    "from dataloader.eegmmidb_labram_physionet_allruns_2class import (\n",
    "    EEGMMIDBLaBraMAllRunsLRDataset,\n",
    "    PreprocConfig,\n",
    "    BCI22_CHANNELS,\n",
    "    LABRAM_64_MAP,\n",
    ")\n",
    "\n",
    "print(\"LaBraM model registered?\", \"labram_base_patch200_200\" in timm.list_models())\n",
    "print(\"Num timm models:\", len(timm.list_models()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a4c439-fea6-46ad-a0e6-b136495558a0",
   "metadata": {},
   "source": [
    "## Dataset: PhysioNet EEGMMIDB (All motor runs → Left vs Right)\n",
    "\n",
    "We load **all left/right motor-related runs** from the PhysioNet EEGMMIDB dataset and **collapse imagined and real movements into a single binary task** (Left vs Right).\n",
    "\n",
    "### Runs used\n",
    "\n",
    "According to the official EEGMMIDB protocol:\n",
    "\n",
    "- **Real (executed) left / right hand**\n",
    "  - Runs **3, 7, 11**\n",
    "\n",
    "- **Imagined left / right hand**\n",
    "  - Runs **4, 8, 12**\n",
    "\n",
    "In this notebook, we use runs:\n",
    "\n",
    "- **4, 8, 12** → left/right hand motor tasks  \n",
    "  (only *real*)\n",
    "\n",
    "### Label mapping\n",
    "\n",
    "- **0 = Left hand** (T1)\n",
    "- **1 = Right hand** (T2)\n",
    "\n",
    "### Notes\n",
    "\n",
    "- Imagined vs real movement is **intentionally ignored** at this stage.\n",
    "- The model is trained to predict **hand laterality only (Left vs Right)**.\n",
    "- Task type (*imagined* vs *real*) is retained in metadata and used **only for post-hoc analysis**, not for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d051922-114c-44a9-bfa5-58b7b1aad8ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/projects/neuro-eeg/dataloader/eegmmidb_labram_physionet_allruns_2class.py:184: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw = mne.io.read_raw_edf(edf_path, preload=True, verbose=False)\n",
      "/home/jovyan/projects/neuro-eeg/dataloader/eegmmidb_labram_physionet_allruns_2class.py:184: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw = mne.io.read_raw_edf(edf_path, preload=True, verbose=False)\n",
      "/home/jovyan/projects/neuro-eeg/dataloader/eegmmidb_labram_physionet_allruns_2class.py:184: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw = mne.io.read_raw_edf(edf_path, preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 4917\n",
      "x shape: torch.Size([22, 4, 200])\n",
      "label: tensor(1)\n",
      "label_name: T2\n",
      "run: 4\n",
      "task_type: imagined\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_ROOT = PROJECT_ROOT / \"data\" / \"physionet.org\" / \"files\" / \"eegmmidb\" / \"1.0.0\"\n",
    "\n",
    "ds = EEGMMIDBLaBraMAllRunsLRDataset(\n",
    "    root_path=str(DATA_ROOT),\n",
    "    subjects=None,          # all subjects\n",
    "    runs=[4, 8, 12],       # imagined + real, left/right\n",
    "    t_min=0.0,\n",
    "    t_max=4.0,              # 4 seconds → 800 samples @ 200 Hz\n",
    "    normalization=True,\n",
    "    is_train=True,\n",
    "    add_noise_std=0.0,\n",
    "    preproc=PreprocConfig(\n",
    "        target_sfreq=200.0,\n",
    "        notch_hz=50.0,\n",
    "        l_freq=0.1,\n",
    "        h_freq=75.0,\n",
    "        reref=\"average\",\n",
    "        to_microvolts=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Dataset length:\", len(ds))\n",
    "\n",
    "sample = ds[0]\n",
    "x = sample[\"inputs\"]\n",
    "y = sample[\"labels\"]\n",
    "meta = sample[\"meta\"]\n",
    "\n",
    "print(\"x shape:\", x.shape)        # (22, 800)\n",
    "print(\"label:\", y)\n",
    "print(\"label_name:\", meta[\"label_name\"])\n",
    "print(\"run:\", meta[\"run\"])\n",
    "print(\"task_type:\", meta[\"task_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee2bb2a9-104e-4828-9c41-0517cdfad3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 4917\n",
      "Runs present: [4, 8, 12]\n",
      "Run counts: {4: 1640, 8: 1637, 12: 1640}\n",
      "\n",
      "Task types present: ['imagined']\n",
      "Task type counts: {'imagined': 4917}\n",
      "\n",
      "Label ids present: [0, 1]\n",
      "Label counts: {0: 2479, 1: 2438}\n",
      "\n",
      "Label_name counts: {'T1': 2479, 'T2': 2438}\n",
      "\n",
      "Missing runs: [3, 7, 11]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "runs = [m[\"run\"] for m in ds.metas]\n",
    "tasks = [m[\"task_type\"] for m in ds.metas]\n",
    "labels = [int(y) for y in ds.labels]\n",
    "label_names = [m[\"label_name\"] for m in ds.metas]\n",
    "\n",
    "print(\"Total samples:\", len(ds))\n",
    "print(\"Runs present:\", sorted(set(runs)))\n",
    "print(\"Run counts:\", dict(sorted(Counter(runs).items())))\n",
    "\n",
    "print(\"\\nTask types present:\", sorted(set(tasks)))\n",
    "print(\"Task type counts:\", dict(sorted(Counter(tasks).items())))\n",
    "\n",
    "print(\"\\nLabel ids present:\", sorted(set(labels)))\n",
    "print(\"Label counts:\", dict(sorted(Counter(labels).items())))\n",
    "\n",
    "print(\"\\nLabel_name counts:\", dict(sorted(Counter(label_names).items())))\n",
    "\n",
    "# sanity: do we have both imagined and real, and all 4 runs?\n",
    "expected_runs = {3, 7, 11}\n",
    "print(\"\\nMissing runs:\", sorted(expected_runs - set(runs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ab1715-62e8-4327-8eae-ca5ed14b9769",
   "metadata": {},
   "source": [
    "## Ensure reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be2bc789-e1f6-400f-bcc1-ba1ffcbd6c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SEED = 55\n",
      "torch initial seed: 55\n"
     ]
    }
   ],
   "source": [
    "# --- Reproducibility guard (run BEFORE dataset split / dataloaders) ---\n",
    "import os, random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = 55  # change if you want a different run, but keep fixed for debugging\n",
    "print(\"Using SEED =\", SEED)\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# CUDNN controls (determinism vs speed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# For PyTorch >= 2.0 (more deterministic, may throw if an op has no deterministic impl)\n",
    "try:\n",
    "    torch.use_deterministic_algorithms(False)  # set True if you want strict determinism (can error)\n",
    "except Exception as e:\n",
    "    print(\"torch.use_deterministic_algorithms not available:\", e)\n",
    "\n",
    "print(\"torch initial seed:\", torch.initial_seed())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dac34dc-bb71-4d19-bf4e-86578e682824",
   "metadata": {},
   "source": [
    "## Train/Val/Test split: split by SUBJECT\n",
    "\n",
    "We must split by **subject**, not by individual windows, otherwise we leak person-specific patterns\n",
    "into validation/test and inflate accuracy.\n",
    "\n",
    "So we will:\n",
    "- pick a set of subjects for train / val / test\n",
    "- build **three datasets** (same preprocessing, different subject lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a5ac54b-9087-4b3b-a9c5-e76617dbd553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num subjects found: 109\n",
      "First 10 subjects: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "\n",
      "Split sizes:\n",
      "train subjects: 87\n",
      "val subjects:   10\n",
      "test subjects:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/projects/neuro-eeg/dataloader/eegmmidb_labram_physionet_allruns_2class.py:184: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw = mne.io.read_raw_edf(edf_path, preload=True, verbose=False)\n",
      "/home/jovyan/projects/neuro-eeg/dataloader/eegmmidb_labram_physionet_allruns_2class.py:184: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw = mne.io.read_raw_edf(edf_path, preload=True, verbose=False)\n",
      "/home/jovyan/projects/neuro-eeg/dataloader/eegmmidb_labram_physionet_allruns_2class.py:184: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw = mne.io.read_raw_edf(edf_path, preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset lengths:\n",
      "train: 3915\n",
      "val:   450\n",
      "test:  552\n"
     ]
    }
   ],
   "source": [
    "from dataloader.eegmmidb_labram_physionet_allruns_2class import EEGMMIDBLaBraMAllRunsLRDataset, PreprocConfig\n",
    "import numpy as np\n",
    "\n",
    "# --- discover all subjects from the dataset we already loaded\n",
    "all_subjects = sorted({m[\"subject\"] for m in ds.metas})\n",
    "print(\"Num subjects found:\", len(all_subjects))\n",
    "print(\"First 10 subjects:\", all_subjects[:10])\n",
    "\n",
    "# --- subject-wise split (80/10/10)\n",
    "rng = np.random.default_rng(SEED)\n",
    "perm = rng.permutation(all_subjects)\n",
    "\n",
    "n = len(perm)\n",
    "n_train = int(0.8 * n)\n",
    "n_val = int(0.1 * n)\n",
    "train_subjects = sorted(perm[:n_train].tolist())\n",
    "val_subjects = sorted(perm[n_train:n_train + n_val].tolist())\n",
    "test_subjects = sorted(perm[n_train + n_val:].tolist())\n",
    "\n",
    "print(\"\\nSplit sizes:\")\n",
    "print(\"train subjects:\", len(train_subjects))\n",
    "print(\"val subjects:  \", len(val_subjects))\n",
    "print(\"test subjects: \", len(test_subjects))\n",
    "\n",
    "# --- reuse the same preprocessing config and run list\n",
    "preproc = PreprocConfig(\n",
    "    target_sfreq=200.0,\n",
    "    notch_hz=50.0,\n",
    "    l_freq=0.1,\n",
    "    h_freq=75.0,\n",
    "    reref=\"average\",\n",
    "    to_microvolts=True,\n",
    ")\n",
    "\n",
    "train_ds = EEGMMIDBLaBraMAllRunsLRDataset(\n",
    "    root_path=str(DATA_ROOT),\n",
    "    subjects=train_subjects,\n",
    "    runs=[4, 8, 12],\n",
    "    t_min=0.0, t_max=4.0,\n",
    "    normalization=True,\n",
    "    is_train=True,\n",
    "    add_noise_std=0.02,   # mild noise aug like before\n",
    "    preproc=preproc,\n",
    ")\n",
    "\n",
    "val_ds = EEGMMIDBLaBraMAllRunsLRDataset(\n",
    "    root_path=str(DATA_ROOT),\n",
    "    subjects=val_subjects,\n",
    "    runs=[4, 8, 12],\n",
    "    t_min=0.0, t_max=4.0,\n",
    "    normalization=True,\n",
    "    is_train=False,\n",
    "    add_noise_std=0.0,\n",
    "    preproc=preproc,\n",
    ")\n",
    "\n",
    "test_ds = EEGMMIDBLaBraMAllRunsLRDataset(\n",
    "    root_path=str(DATA_ROOT),\n",
    "    subjects=test_subjects,\n",
    "    runs=[4, 8, 12],\n",
    "    t_min=0.0, t_max=4.0,\n",
    "    normalization=True,\n",
    "    is_train=False,\n",
    "    add_noise_std=0.0,\n",
    "    preproc=preproc,\n",
    ")\n",
    "\n",
    "print(\"\\nDataset lengths:\")\n",
    "print(\"train:\", len(train_ds))\n",
    "print(\"val:  \", len(val_ds))\n",
    "print(\"test: \", len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73059a3b-b4ba-40e9-be15-73ff45aa811a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch inputs: (32, 22, 4, 200) Batch labels: (32,)\n",
      "Label min/max: 0 1\n",
      "meta type: <class 'dict'>\n",
      "meta keys: ['subject', 'run', 'task_type', 'label_name', 'sfreq', 'ch_names', 't_min', 't_max', 'path']\n",
      "\n",
      "Example meta field 'subject' (first 5): tensor([ 10,  50,  37, 101,  75])\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Keep notebook output readable\n",
    "warnings.filterwarnings(\"ignore\", message=\"NOTE: pick_channels\\\\(\\\\) is a legacy function.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Limited .* annotation\\\\(s\\\\).*expanding outside the data range.*\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 6  # adjust if your JupyterHub is tight\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# quick sanity batch\n",
    "batch = next(iter(train_loader))\n",
    "print(\"Batch inputs:\", tuple(batch[\"inputs\"].shape), \"Batch labels:\", tuple(batch[\"labels\"].shape))\n",
    "print(\"Label min/max:\", int(batch[\"labels\"].min()), int(batch[\"labels\"].max()))\n",
    "\n",
    "meta = batch[\"meta\"]\n",
    "\n",
    "print(\"meta type:\", type(meta))\n",
    "print(\"meta keys:\", list(meta.keys()))\n",
    "\n",
    "# show one meta field (first 5 entries)\n",
    "any_key = list(meta.keys())[0]\n",
    "print(f\"\\nExample meta field '{any_key}' (first 5):\", meta[any_key][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f19b7d6-cd5c-49b4-b8a7-0c91cf042c31",
   "metadata": {},
   "source": [
    "## LaBraM channel mapping (22 channels → LaBraM positional indices)\n",
    "\n",
    "LaBraM was trained with a channel-position embedding table, so we pass an `input_chans` index list.\n",
    "\n",
    "Key detail:\n",
    "- LaBraM expects `input_chans` to include a dummy **0** index at the front (for the CLS token position usage in their code).\n",
    "- Then we add the 22 mapped channel indices (from `LABRAM_64_MAP`).\n",
    "\n",
    "So `input_chans` will have length **23**:\n",
    "`[0] + [mapped indices for the 22 BCI channels]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3ca0c57-8f60-4765-a5af-17b629a77f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_chans shape: (23,) first 10: [0, 2, 10, 11, 12, 13, 14, 26, 27, 28]\n",
      "Loading checkpoint: /home/jovyan/projects/neuro-eeg/LaBraM/checkpoints/labram-base.pth\n",
      "Stripped 'student.' prefix.\n",
      "Checkpoint loaded. Missing: 28 Unexpected: 5\n",
      "Missing (first 8): ['blocks.0.attn.q_bias', 'blocks.0.attn.v_bias', 'blocks.1.attn.q_bias', 'blocks.1.attn.v_bias', 'blocks.2.attn.q_bias', 'blocks.2.attn.v_bias', 'blocks.3.attn.q_bias', 'blocks.3.attn.v_bias']\n",
      "Unexpected (first 8): ['mask_token', 'lm_head.weight', 'lm_head.bias', 'norm.weight', 'norm.bias']\n",
      "Forward OK. logits shape: (2, 2)\n",
      "logits: tensor([[ 2.7576e-05,  1.3205e-04],\n",
      "        [-5.7700e-05,  8.1034e-05]])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import timm\n",
    "\n",
    "# --- Ensure LaBraM is importable + registers models\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "LABRAM_ROOT = PROJECT_ROOT / \"LaBraM\"\n",
    "if str(LABRAM_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(LABRAM_ROOT))\n",
    "\n",
    "import modeling_finetune  # registers into timm\n",
    "\n",
    "# --- input_chans: [0] + 22 mapped indices\n",
    "mapped = [LABRAM_64_MAP[ch] for ch in BCI22_CHANNELS]\n",
    "input_chans = torch.tensor([0] + mapped, dtype=torch.long)  # (23,)\n",
    "print(\"input_chans shape:\", tuple(input_chans.shape), \"first 10:\", input_chans[:10].tolist())\n",
    "\n",
    "# --- Build LaBraM model for binary classification\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = timm.create_model(\n",
    "    \"labram_base_patch200_200\",\n",
    "    pretrained=False,\n",
    "    num_classes=2,\n",
    "    drop_rate=0.0,\n",
    "    drop_path_rate=0.1,\n",
    "    attn_drop_rate=0.0,\n",
    "    drop_block_rate=None,\n",
    "    use_mean_pooling=True,\n",
    "    init_scale=0.001,\n",
    "    use_rel_pos_bias=True,\n",
    "    use_abs_pos_emb=True,\n",
    "    init_values=0.1,\n",
    "    qkv_bias=True,\n",
    ").to(device)\n",
    "\n",
    "# --- Load pretrained checkpoint (labram-base.pth)\n",
    "ckpt_path = PROJECT_ROOT / \"LaBraM\" / \"checkpoints\" / \"labram-base.pth\"\n",
    "print(\"Loading checkpoint:\", ckpt_path)\n",
    "\n",
    "checkpoint = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "checkpoint_model = checkpoint.get(\"model\", checkpoint)\n",
    "\n",
    "# Strip 'student.' prefix if present\n",
    "if isinstance(checkpoint_model, dict) and any(k.startswith(\"student.\") for k in checkpoint_model.keys()):\n",
    "    new_dict = OrderedDict()\n",
    "    for k, v in checkpoint_model.items():\n",
    "        if k.startswith(\"student.\"):\n",
    "            new_dict[k[len(\"student.\"):]] = v\n",
    "    checkpoint_model = new_dict\n",
    "    print(\"Stripped 'student.' prefix.\")\n",
    "\n",
    "# Remove classifier head if mismatched\n",
    "state_dict = model.state_dict()\n",
    "for k in [\"head.weight\", \"head.bias\"]:\n",
    "    if k in checkpoint_model and k in state_dict and checkpoint_model[k].shape != state_dict[k].shape:\n",
    "        del checkpoint_model[k]\n",
    "\n",
    "# Remove relative_position_index buffers if present\n",
    "for k in list(checkpoint_model.keys()):\n",
    "    if \"relative_position_index\" in k:\n",
    "        checkpoint_model.pop(k)\n",
    "\n",
    "missing, unexpected = model.load_state_dict(checkpoint_model, strict=False)\n",
    "print(\"Checkpoint loaded. Missing:\", len(missing), \"Unexpected:\", len(unexpected))\n",
    "print(\"Missing (first 8):\", missing[:8])\n",
    "print(\"Unexpected (first 8):\", unexpected[:8])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# --- Forward pass sanity on one batch\n",
    "batch = next(iter(train_loader))\n",
    "x = batch[\"inputs\"][:2].to(device)  # (B, 22, 4, 200)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(x, input_chans=input_chans.to(device))\n",
    "\n",
    "print(\"Forward OK. logits shape:\", tuple(logits.shape))\n",
    "print(\"logits:\", logits.detach().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bc19ed-0d04-41c0-81b2-b457c4238f2e",
   "metadata": {},
   "source": [
    "## Training setup (binary Left vs Right)\n",
    "\n",
    "Before training, we rebuild the model with `qkv_bias=False` so that checkpoint loading matches\n",
    "and we avoid the extra \"missing q_bias/v_bias\" keys.\n",
    "\n",
    "Then we:\n",
    "- set up optimizer (AdamW)\n",
    "- set up a simple training loop with validation accuracy\n",
    "- save the best model to `training_logs/labram_physionet_lr_allruns/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "807b9e6f-140d-40ff-a065-5da0547fe42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT_DIR: /home/jovyan/projects/neuro-eeg/training_logs/labram_physionet_lr_allruns\n",
      "Checkpoint loaded. Missing: 4 Unexpected: 5\n",
      "Missing (first 10): ['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias']\n",
      "Unexpected (first 10): ['mask_token', 'lm_head.weight', 'lm_head.bias', 'norm.weight', 'norm.bias']\n",
      "Ready to train. Example param count: 5.820338 M params\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import timm\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "OUT_DIR = Path.cwd() / \"training_logs\" / \"labram_physionet_imagined_allruns\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Build model (match checkpoint better by disabling qkv_bias)\n",
    "model = timm.create_model(\n",
    "    \"labram_base_patch200_200\",\n",
    "    pretrained=False,\n",
    "    num_classes=2,\n",
    "    drop_rate=0.0,\n",
    "    drop_path_rate=0.1,\n",
    "    attn_drop_rate=0.0,\n",
    "    drop_block_rate=None,\n",
    "    use_mean_pooling=True,\n",
    "    init_scale=0.001,\n",
    "    use_rel_pos_bias=True,\n",
    "    use_abs_pos_emb=True,\n",
    "    init_values=0.1,\n",
    "    qkv_bias=False,   # <--- important\n",
    ").to(device)\n",
    "\n",
    "# --- Load checkpoint again\n",
    "ckpt_path = Path.cwd() / \"LaBraM\" / \"checkpoints\" / \"labram-base.pth\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n",
    "checkpoint_model = checkpoint.get(\"model\", checkpoint)\n",
    "\n",
    "# Strip 'student.' prefix if present\n",
    "if isinstance(checkpoint_model, dict) and any(k.startswith(\"student.\") for k in checkpoint_model.keys()):\n",
    "    new_dict = OrderedDict()\n",
    "    for k, v in checkpoint_model.items():\n",
    "        if k.startswith(\"student.\"):\n",
    "            new_dict[k[len(\"student.\"):]] = v\n",
    "    checkpoint_model = new_dict\n",
    "\n",
    "# Remove classifier head if mismatched\n",
    "state_dict = model.state_dict()\n",
    "for k in [\"head.weight\", \"head.bias\"]:\n",
    "    if k in checkpoint_model and k in state_dict and checkpoint_model[k].shape != state_dict[k].shape:\n",
    "        del checkpoint_model[k]\n",
    "\n",
    "# Remove relative_position_index buffers if present\n",
    "for k in list(checkpoint_model.keys()):\n",
    "    if \"relative_position_index\" in k:\n",
    "        checkpoint_model.pop(k)\n",
    "\n",
    "missing, unexpected = model.load_state_dict(checkpoint_model, strict=False)\n",
    "print(\"Checkpoint loaded. Missing:\", len(missing), \"Unexpected:\", len(unexpected))\n",
    "print(\"Missing (first 10):\", missing[:10])\n",
    "print(\"Unexpected (first 10):\", unexpected[:10])\n",
    "\n",
    "# --- Optimizer\n",
    "lr = 5e-4\n",
    "weight_decay = 0.05\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Ready to train. Example param count:\", sum(p.numel() for p in model.parameters())/1e6, \"M params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17778161-913a-419a-adfb-f09c7cbd33fb",
   "metadata": {},
   "source": [
    "## Train LaBraM on PhysioNet EEGMMIDB (runs 4,8,12) — Left vs Right\n",
    "\n",
    "We will train for a modest number of epochs first (e.g. 10),\n",
    "track validation accuracy, and save the best checkpoint to:\n",
    "\n",
    "`training_logs/labram_physionet_lr_allruns/best.pth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57e107ce-58a9-4fe5-97e1-db7fec86b70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01 step 0000 | loss 0.6931\n",
      "epoch 01 step 0100 | loss 0.7011\n",
      "epoch 01 | train loss 0.6953 acc 0.4987 | val loss 0.6935 acc 0.4956 | best 0.4956 | 5.7s\n",
      "epoch 02 step 0000 | loss 0.6928\n",
      "epoch 02 step 0100 | loss 0.6988\n",
      "epoch 02 | train loss 0.6957 acc 0.4831 | val loss 0.6931 acc 0.5044 | best 0.5044 | 5.5s\n",
      "epoch 03 step 0000 | loss 0.6922\n",
      "epoch 03 step 0100 | loss 0.6917\n",
      "epoch 03 | train loss 0.6952 acc 0.4926 | val loss 0.6933 acc 0.4956 | best 0.5044 | 5.3s\n",
      "epoch 04 step 0000 | loss 0.6954\n",
      "epoch 04 step 0100 | loss 0.6952\n",
      "epoch 04 | train loss 0.6941 acc 0.4946 | val loss 0.6931 acc 0.5044 | best 0.5044 | 5.2s\n",
      "epoch 05 step 0000 | loss 0.6925\n",
      "epoch 05 step 0100 | loss 0.6933\n",
      "epoch 05 | train loss 0.6940 acc 0.4990 | val loss 0.6937 acc 0.4956 | best 0.5044 | 5.3s\n",
      "epoch 06 step 0000 | loss 0.6857\n",
      "epoch 06 step 0100 | loss 0.6947\n",
      "epoch 06 | train loss 0.6936 acc 0.5100 | val loss 0.6932 acc 0.5044 | best 0.5044 | 5.3s\n",
      "epoch 07 step 0000 | loss 0.6880\n",
      "epoch 07 step 0100 | loss 0.6931\n",
      "epoch 07 | train loss 0.6939 acc 0.4980 | val loss 0.6932 acc 0.5044 | best 0.5044 | 5.3s\n",
      "epoch 08 step 0000 | loss 0.6954\n",
      "epoch 08 step 0100 | loss 0.6952\n",
      "epoch 08 | train loss 0.6935 acc 0.4987 | val loss 0.6939 acc 0.4956 | best 0.5044 | 5.4s\n",
      "epoch 09 step 0000 | loss 0.6938\n",
      "epoch 09 step 0100 | loss 0.6788\n",
      "epoch 09 | train loss 0.6935 acc 0.5074 | val loss 0.6931 acc 0.5044 | best 0.5044 | 5.4s\n",
      "epoch 10 step 0000 | loss 0.6934\n",
      "epoch 10 step 0100 | loss 0.6911\n",
      "epoch 10 | train loss 0.6942 acc 0.4926 | val loss 0.6933 acc 0.4956 | best 0.5044 | 5.3s\n",
      "epoch 11 step 0000 | loss 0.6933\n",
      "epoch 11 step 0100 | loss 0.6955\n",
      "epoch 11 | train loss 0.6935 acc 0.4959 | val loss 0.6937 acc 0.4956 | best 0.5044 | 5.5s\n",
      "epoch 12 step 0000 | loss 0.6954\n",
      "epoch 12 step 0100 | loss 0.6926\n",
      "epoch 12 | train loss 0.6937 acc 0.4928 | val loss 0.6931 acc 0.5044 | best 0.5044 | 5.3s\n",
      "epoch 13 step 0000 | loss 0.6920\n",
      "epoch 13 step 0100 | loss 0.6957\n",
      "epoch 13 | train loss 0.6936 acc 0.4951 | val loss 0.6931 acc 0.5044 | best 0.5044 | 5.4s\n",
      "epoch 14 step 0000 | loss 0.6915\n",
      "epoch 14 step 0100 | loss 0.6934\n",
      "epoch 14 | train loss 0.6933 acc 0.5010 | val loss 0.6932 acc 0.5044 | best 0.5044 | 5.7s\n",
      "epoch 15 step 0000 | loss 0.6958\n",
      "epoch 15 step 0100 | loss 0.6899\n",
      "epoch 15 | train loss 0.6935 acc 0.5013 | val loss 0.6931 acc 0.5044 | best 0.5044 | 5.3s\n",
      "epoch 16 step 0000 | loss 0.6924\n",
      "epoch 16 step 0100 | loss 0.6908\n",
      "epoch 16 | train loss 0.6937 acc 0.5000 | val loss 0.6931 acc 0.5044 | best 0.5044 | 5.2s\n",
      "epoch 17 step 0000 | loss 0.6926\n",
      "epoch 17 step 0100 | loss 0.6891\n",
      "epoch 17 | train loss 0.6933 acc 0.5077 | val loss 0.6950 acc 0.5044 | best 0.5044 | 5.2s\n",
      "epoch 18 step 0000 | loss 0.6871\n",
      "epoch 18 step 0100 | loss 0.6930\n",
      "epoch 18 | train loss 0.6936 acc 0.5020 | val loss 0.6930 acc 0.5044 | best 0.5044 | 5.3s\n",
      "epoch 19 step 0000 | loss 0.6947\n",
      "epoch 19 step 0100 | loss 0.7037\n",
      "epoch 19 | train loss 0.6884 acc 0.5392 | val loss 0.6937 acc 0.4956 | best 0.5044 | 5.3s\n",
      "epoch 20 step 0000 | loss 0.7036\n",
      "epoch 20 step 0100 | loss 0.6971\n",
      "epoch 20 | train loss 0.6934 acc 0.5090 | val loss 0.6933 acc 0.4956 | best 0.5044 | 5.2s\n",
      "epoch 21 step 0000 | loss 0.6960\n",
      "epoch 21 step 0100 | loss 0.6966\n",
      "epoch 21 | train loss 0.6938 acc 0.4926 | val loss 0.6931 acc 0.5044 | best 0.5044 | 5.3s\n",
      "epoch 22 step 0000 | loss 0.6902\n",
      "epoch 22 step 0100 | loss 0.6957\n",
      "epoch 22 | train loss 0.6939 acc 0.4900 | val loss 0.6931 acc 0.5044 | best 0.5044 | 5.3s\n",
      "epoch 23 step 0000 | loss 0.6971\n",
      "epoch 23 step 0100 | loss 0.6767\n",
      "epoch 23 | train loss 0.6934 acc 0.5003 | val loss 0.6931 acc 0.5044 | best 0.5044 | 5.2s\n",
      "epoch 24 step 0000 | loss 0.6906\n",
      "epoch 24 step 0100 | loss 0.6927\n",
      "epoch 24 | train loss 0.6934 acc 0.4982 | val loss 0.6933 acc 0.4956 | best 0.5044 | 5.2s\n",
      "epoch 25 step 0000 | loss 0.6878\n",
      "epoch 25 step 0100 | loss 0.6951\n",
      "epoch 25 | train loss 0.6935 acc 0.4951 | val loss 0.6934 acc 0.4956 | best 0.5044 | 5.2s\n",
      "epoch 26 step 0000 | loss 0.6956\n",
      "epoch 26 step 0100 | loss 0.6937\n",
      "epoch 26 | train loss 0.6935 acc 0.5044 | val loss 0.6961 acc 0.5044 | best 0.5044 | 5.2s\n",
      "epoch 27 step 0000 | loss 0.6966\n",
      "epoch 27 step 0100 | loss 0.6886\n",
      "epoch 27 | train loss 0.6941 acc 0.4951 | val loss 0.6927 acc 0.4956 | best 0.5044 | 5.3s\n",
      "epoch 28 step 0000 | loss 0.6945\n",
      "epoch 28 step 0100 | loss 0.6877\n",
      "epoch 28 | train loss 0.6859 acc 0.5448 | val loss 0.6933 acc 0.4956 | best 0.5044 | 5.2s\n",
      "epoch 29 step 0000 | loss 0.6995\n",
      "epoch 29 step 0100 | loss 0.6826\n",
      "epoch 29 | train loss 0.6894 acc 0.5405 | val loss 0.6762 acc 0.6022 | best 0.6022 | 5.4s\n",
      "epoch 30 step 0000 | loss 0.6649\n",
      "epoch 30 step 0100 | loss 0.6601\n",
      "epoch 30 | train loss 0.6396 acc 0.6429 | val loss 0.5851 acc 0.6911 | best 0.6911 | 5.4s\n",
      "epoch 31 step 0000 | loss 0.6922\n",
      "epoch 31 step 0100 | loss 0.6797\n",
      "epoch 31 | train loss 0.5558 acc 0.7170 | val loss 0.5421 acc 0.7333 | best 0.7333 | 5.4s\n",
      "epoch 32 step 0000 | loss 0.4668\n",
      "epoch 32 step 0100 | loss 0.6334\n",
      "epoch 32 | train loss 0.5108 acc 0.7520 | val loss 0.5339 acc 0.7422 | best 0.7422 | 5.4s\n",
      "epoch 33 step 0000 | loss 0.7027\n",
      "epoch 33 step 0100 | loss 0.3513\n",
      "epoch 33 | train loss 0.4724 acc 0.7777 | val loss 0.4851 acc 0.7689 | best 0.7689 | 5.5s\n",
      "epoch 34 step 0000 | loss 0.3641\n",
      "epoch 34 step 0100 | loss 0.3312\n",
      "epoch 34 | train loss 0.4561 acc 0.7820 | val loss 0.4781 acc 0.7556 | best 0.7689 | 5.5s\n",
      "epoch 35 step 0000 | loss 0.4300\n",
      "epoch 35 step 0100 | loss 0.4089\n",
      "epoch 35 | train loss 0.4419 acc 0.7910 | val loss 0.4957 acc 0.7444 | best 0.7689 | 5.2s\n",
      "epoch 36 step 0000 | loss 0.4422\n",
      "epoch 36 step 0100 | loss 0.3292\n",
      "epoch 36 | train loss 0.4323 acc 0.7948 | val loss 0.4725 acc 0.7800 | best 0.7800 | 5.4s\n",
      "epoch 37 step 0000 | loss 0.4227\n",
      "epoch 37 step 0100 | loss 0.4427\n",
      "epoch 37 | train loss 0.4221 acc 0.8015 | val loss 0.4878 acc 0.7289 | best 0.7800 | 5.2s\n",
      "epoch 38 step 0000 | loss 0.4121\n",
      "epoch 38 step 0100 | loss 0.5604\n",
      "epoch 38 | train loss 0.4140 acc 0.8135 | val loss 0.4864 acc 0.7556 | best 0.7800 | 5.2s\n",
      "epoch 39 step 0000 | loss 0.4745\n",
      "epoch 39 step 0100 | loss 0.4050\n",
      "epoch 39 | train loss 0.3941 acc 0.8169 | val loss 0.4916 acc 0.7533 | best 0.7800 | 5.3s\n",
      "epoch 40 step 0000 | loss 0.4447\n",
      "epoch 40 step 0100 | loss 0.2961\n",
      "epoch 40 | train loss 0.3824 acc 0.8233 | val loss 0.5301 acc 0.7378 | best 0.7800 | 5.3s\n",
      "epoch 41 step 0000 | loss 0.3178\n",
      "epoch 41 step 0100 | loss 0.2133\n",
      "epoch 41 | train loss 0.3653 acc 0.8289 | val loss 0.4992 acc 0.7400 | best 0.7800 | 5.3s\n",
      "epoch 42 step 0000 | loss 0.4405\n",
      "epoch 42 step 0100 | loss 0.3267\n",
      "epoch 42 | train loss 0.3528 acc 0.8417 | val loss 0.5834 acc 0.7444 | best 0.7800 | 5.6s\n",
      "epoch 43 step 0000 | loss 0.5163\n",
      "epoch 43 step 0100 | loss 0.3043\n",
      "epoch 43 | train loss 0.3301 acc 0.8512 | val loss 0.6496 acc 0.7378 | best 0.7800 | 5.2s\n",
      "epoch 44 step 0000 | loss 0.2120\n",
      "epoch 44 step 0100 | loss 0.4543\n",
      "epoch 44 | train loss 0.3147 acc 0.8645 | val loss 0.6282 acc 0.7422 | best 0.7800 | 5.3s\n",
      "epoch 45 step 0000 | loss 0.3383\n",
      "epoch 45 step 0100 | loss 0.3415\n",
      "epoch 45 | train loss 0.3100 acc 0.8617 | val loss 0.5928 acc 0.7489 | best 0.7800 | 5.3s\n",
      "epoch 46 step 0000 | loss 0.3331\n",
      "epoch 46 step 0100 | loss 0.3225\n",
      "epoch 46 | train loss 0.2820 acc 0.8763 | val loss 0.6076 acc 0.7267 | best 0.7800 | 5.3s\n",
      "epoch 47 step 0000 | loss 0.2900\n",
      "epoch 47 step 0100 | loss 0.1479\n",
      "epoch 47 | train loss 0.2564 acc 0.8934 | val loss 0.6680 acc 0.7400 | best 0.7800 | 5.3s\n",
      "epoch 48 step 0000 | loss 0.1580\n",
      "epoch 48 step 0100 | loss 0.3988\n",
      "epoch 48 | train loss 0.2343 acc 0.9098 | val loss 0.7153 acc 0.7200 | best 0.7800 | 5.4s\n",
      "epoch 49 step 0000 | loss 0.1463\n",
      "epoch 49 step 0100 | loss 0.3322\n",
      "epoch 49 | train loss 0.2143 acc 0.9183 | val loss 0.7342 acc 0.7178 | best 0.7800 | 5.3s\n",
      "epoch 50 step 0000 | loss 0.2736\n",
      "epoch 50 step 0100 | loss 0.2903\n",
      "epoch 50 | train loss 0.2080 acc 0.9168 | val loss 0.7483 acc 0.7356 | best 0.7800 | 5.3s\n",
      "epoch 51 step 0000 | loss 0.1708\n",
      "epoch 51 step 0100 | loss 0.0896\n",
      "epoch 51 | train loss 0.1853 acc 0.9288 | val loss 0.8651 acc 0.7133 | best 0.7800 | 5.3s\n",
      "epoch 52 step 0000 | loss 0.2223\n",
      "epoch 52 step 0100 | loss 0.2437\n",
      "epoch 52 | train loss 0.1766 acc 0.9344 | val loss 1.0089 acc 0.7156 | best 0.7800 | 5.3s\n",
      "epoch 53 step 0000 | loss 0.0759\n",
      "epoch 53 step 0100 | loss 0.1613\n",
      "epoch 53 | train loss 0.1651 acc 0.9411 | val loss 0.7809 acc 0.7067 | best 0.7800 | 5.3s\n",
      "epoch 54 step 0000 | loss 0.1705\n",
      "epoch 54 step 0100 | loss 0.0606\n",
      "epoch 54 | train loss 0.1291 acc 0.9531 | val loss 1.0309 acc 0.7000 | best 0.7800 | 5.3s\n",
      "epoch 55 step 0000 | loss 0.0260\n",
      "epoch 55 step 0100 | loss 0.1269\n",
      "epoch 55 | train loss 0.1197 acc 0.9606 | val loss 1.0679 acc 0.7133 | best 0.7800 | 5.3s\n",
      "epoch 56 step 0000 | loss 0.0443\n",
      "epoch 56 step 0100 | loss 0.2137\n",
      "epoch 56 | train loss 0.1206 acc 0.9570 | val loss 0.9117 acc 0.7200 | best 0.7800 | 5.5s\n",
      "epoch 57 step 0000 | loss 0.2325\n",
      "epoch 57 step 0100 | loss 0.0264\n",
      "epoch 57 | train loss 0.0972 acc 0.9657 | val loss 1.0274 acc 0.7267 | best 0.7800 | 5.3s\n",
      "epoch 58 step 0000 | loss 0.0926\n",
      "epoch 58 step 0100 | loss 0.0928\n",
      "epoch 58 | train loss 0.0953 acc 0.9690 | val loss 0.9984 acc 0.7289 | best 0.7800 | 5.3s\n",
      "epoch 59 step 0000 | loss 0.0501\n",
      "epoch 59 step 0100 | loss 0.0990\n",
      "epoch 59 | train loss 0.1509 acc 0.9480 | val loss 1.0748 acc 0.7111 | best 0.7800 | 5.3s\n",
      "epoch 60 step 0000 | loss 0.0705\n",
      "epoch 60 step 0100 | loss 0.0506\n",
      "epoch 60 | train loss 0.0927 acc 0.9700 | val loss 1.0577 acc 0.7111 | best 0.7800 | 5.2s\n",
      "epoch 61 step 0000 | loss 0.0415\n",
      "epoch 61 step 0100 | loss 0.2130\n",
      "epoch 61 | train loss 0.0884 acc 0.9746 | val loss 1.0851 acc 0.7222 | best 0.7800 | 5.3s\n",
      "epoch 62 step 0000 | loss 0.0718\n",
      "epoch 62 step 0100 | loss 0.0120\n",
      "epoch 62 | train loss 0.0807 acc 0.9695 | val loss 1.3024 acc 0.7022 | best 0.7800 | 5.3s\n",
      "epoch 63 step 0000 | loss 0.0290\n",
      "epoch 63 step 0100 | loss 0.0576\n",
      "epoch 63 | train loss 0.0639 acc 0.9790 | val loss 1.1029 acc 0.7244 | best 0.7800 | 5.3s\n",
      "epoch 64 step 0000 | loss 0.0113\n",
      "epoch 64 step 0100 | loss 0.0222\n",
      "epoch 64 | train loss 0.0535 acc 0.9844 | val loss 1.2465 acc 0.7111 | best 0.7800 | 5.3s\n",
      "epoch 65 step 0000 | loss 0.0142\n",
      "epoch 65 step 0100 | loss 0.2522\n",
      "epoch 65 | train loss 0.0619 acc 0.9810 | val loss 1.2378 acc 0.6911 | best 0.7800 | 5.3s\n",
      "epoch 66 step 0000 | loss 0.1608\n",
      "epoch 66 step 0100 | loss 0.0551\n",
      "epoch 66 | train loss 0.0692 acc 0.9785 | val loss 1.1612 acc 0.7044 | best 0.7800 | 5.6s\n",
      "epoch 67 step 0000 | loss 0.0192\n",
      "epoch 67 step 0100 | loss 0.2808\n",
      "epoch 67 | train loss 0.0601 acc 0.9810 | val loss 1.2417 acc 0.6978 | best 0.7800 | 5.2s\n",
      "epoch 68 step 0000 | loss 0.0130\n",
      "epoch 68 step 0100 | loss 0.2153\n",
      "epoch 68 | train loss 0.0616 acc 0.9803 | val loss 1.3516 acc 0.6889 | best 0.7800 | 5.2s\n",
      "epoch 69 step 0000 | loss 0.1885\n",
      "epoch 69 step 0100 | loss 0.0062\n",
      "epoch 69 | train loss 0.0488 acc 0.9844 | val loss 1.3185 acc 0.6956 | best 0.7800 | 5.2s\n",
      "epoch 70 step 0000 | loss 0.0339\n",
      "epoch 70 step 0100 | loss 0.0070\n",
      "epoch 70 | train loss 0.0492 acc 0.9851 | val loss 1.3368 acc 0.7156 | best 0.7800 | 5.3s\n",
      "epoch 71 step 0000 | loss 0.0205\n",
      "epoch 71 step 0100 | loss 0.0965\n",
      "epoch 71 | train loss 0.0449 acc 0.9864 | val loss 1.4413 acc 0.7222 | best 0.7800 | 5.2s\n",
      "epoch 72 step 0000 | loss 0.0067\n",
      "epoch 72 step 0100 | loss 0.0205\n",
      "epoch 72 | train loss 0.0359 acc 0.9887 | val loss 1.6042 acc 0.7089 | best 0.7800 | 5.3s\n",
      "epoch 73 step 0000 | loss 0.0045\n",
      "epoch 73 step 0100 | loss 0.1094\n",
      "epoch 73 | train loss 0.0436 acc 0.9872 | val loss 1.2553 acc 0.6911 | best 0.7800 | 5.3s\n",
      "epoch 74 step 0000 | loss 0.1292\n",
      "epoch 74 step 0100 | loss 0.0068\n",
      "epoch 74 | train loss 0.0527 acc 0.9828 | val loss 1.4642 acc 0.7133 | best 0.7800 | 5.3s\n",
      "epoch 75 step 0000 | loss 0.1544\n",
      "epoch 75 step 0100 | loss 0.0152\n",
      "epoch 75 | train loss 0.0468 acc 0.9857 | val loss 1.7153 acc 0.6800 | best 0.7800 | 5.3s\n",
      "epoch 76 step 0000 | loss 0.1091\n",
      "epoch 76 step 0100 | loss 0.0068\n",
      "epoch 76 | train loss 0.0611 acc 0.9818 | val loss 1.5598 acc 0.7022 | best 0.7800 | 5.3s\n",
      "epoch 77 step 0000 | loss 0.0067\n",
      "epoch 77 step 0100 | loss 0.1817\n",
      "epoch 77 | train loss 0.0497 acc 0.9836 | val loss 1.2971 acc 0.7267 | best 0.7800 | 5.3s\n",
      "epoch 78 step 0000 | loss 0.0096\n",
      "epoch 78 step 0100 | loss 0.0180\n",
      "epoch 78 | train loss 0.0356 acc 0.9900 | val loss 1.4832 acc 0.7267 | best 0.7800 | 5.3s\n",
      "epoch 79 step 0000 | loss 0.0074\n",
      "epoch 79 step 0100 | loss 0.0027\n",
      "epoch 79 | train loss 0.0323 acc 0.9915 | val loss 1.6222 acc 0.7067 | best 0.7800 | 5.3s\n",
      "epoch 80 step 0000 | loss 0.0010\n",
      "epoch 80 step 0100 | loss 0.1198\n",
      "epoch 80 | train loss 0.0531 acc 0.9849 | val loss 1.4668 acc 0.7044 | best 0.7800 | 5.3s\n",
      "epoch 81 step 0000 | loss 0.0026\n",
      "epoch 81 step 0100 | loss 0.0050\n",
      "epoch 81 | train loss 0.0453 acc 0.9846 | val loss 1.5799 acc 0.7089 | best 0.7800 | 5.3s\n",
      "epoch 82 step 0000 | loss 0.0078\n",
      "epoch 82 step 0100 | loss 0.0138\n",
      "epoch 82 | train loss 0.0347 acc 0.9874 | val loss 1.6574 acc 0.7133 | best 0.7800 | 5.4s\n",
      "epoch 83 step 0000 | loss 0.0136\n",
      "epoch 83 step 0100 | loss 0.0500\n",
      "epoch 83 | train loss 0.0219 acc 0.9936 | val loss 1.8682 acc 0.7067 | best 0.7800 | 5.3s\n",
      "epoch 84 step 0000 | loss 0.0111\n",
      "epoch 84 step 0100 | loss 0.0085\n",
      "epoch 84 | train loss 0.0356 acc 0.9877 | val loss 1.6865 acc 0.7044 | best 0.7800 | 5.2s\n",
      "epoch 85 step 0000 | loss 0.0013\n",
      "epoch 85 step 0100 | loss 0.0267\n",
      "epoch 85 | train loss 0.0452 acc 0.9818 | val loss 1.9075 acc 0.7000 | best 0.7800 | 5.3s\n",
      "epoch 86 step 0000 | loss 0.0006\n",
      "epoch 86 step 0100 | loss 0.0038\n",
      "epoch 86 | train loss 0.0384 acc 0.9867 | val loss 1.7895 acc 0.6978 | best 0.7800 | 5.3s\n",
      "epoch 87 step 0000 | loss 0.0048\n",
      "epoch 87 step 0100 | loss 0.1263\n",
      "epoch 87 | train loss 0.0728 acc 0.9764 | val loss 1.3619 acc 0.7044 | best 0.7800 | 5.2s\n",
      "epoch 88 step 0000 | loss 0.0735\n",
      "epoch 88 step 0100 | loss 0.0198\n",
      "epoch 88 | train loss 0.0410 acc 0.9877 | val loss 1.3934 acc 0.7200 | best 0.7800 | 5.3s\n",
      "epoch 89 step 0000 | loss 0.0414\n",
      "epoch 89 step 0100 | loss 0.0027\n",
      "epoch 89 | train loss 0.0240 acc 0.9903 | val loss 1.6736 acc 0.7156 | best 0.7800 | 5.3s\n",
      "epoch 90 step 0000 | loss 0.0008\n",
      "epoch 90 step 0100 | loss 0.0100\n",
      "epoch 90 | train loss 0.0263 acc 0.9905 | val loss 1.6042 acc 0.7178 | best 0.7800 | 5.5s\n",
      "epoch 91 step 0000 | loss 0.0152\n",
      "epoch 91 step 0100 | loss 0.1807\n",
      "epoch 91 | train loss 0.0583 acc 0.9787 | val loss 1.4228 acc 0.7156 | best 0.7800 | 5.3s\n",
      "epoch 92 step 0000 | loss 0.0256\n",
      "epoch 92 step 0100 | loss 0.0024\n",
      "epoch 92 | train loss 0.0280 acc 0.9908 | val loss 1.4619 acc 0.7111 | best 0.7800 | 5.2s\n",
      "epoch 93 step 0000 | loss 0.0013\n",
      "epoch 93 step 0100 | loss 0.0153\n",
      "epoch 93 | train loss 0.0254 acc 0.9913 | val loss 1.6456 acc 0.6756 | best 0.7800 | 5.3s\n",
      "epoch 94 step 0000 | loss 0.0952\n",
      "epoch 94 step 0100 | loss 0.0052\n",
      "epoch 94 | train loss 0.0238 acc 0.9928 | val loss 1.7819 acc 0.7022 | best 0.7800 | 5.5s\n",
      "epoch 95 step 0000 | loss 0.0012\n",
      "epoch 95 step 0100 | loss 0.0012\n",
      "epoch 95 | train loss 0.0270 acc 0.9890 | val loss 1.8447 acc 0.6778 | best 0.7800 | 5.3s\n",
      "epoch 96 step 0000 | loss 0.0353\n",
      "epoch 96 step 0100 | loss 0.0356\n",
      "epoch 96 | train loss 0.0429 acc 0.9818 | val loss 1.2638 acc 0.7222 | best 0.7800 | 5.3s\n",
      "epoch 97 step 0000 | loss 0.1010\n",
      "epoch 97 step 0100 | loss 0.0015\n",
      "epoch 97 | train loss 0.0218 acc 0.9936 | val loss 1.7194 acc 0.7067 | best 0.7800 | 5.3s\n",
      "epoch 98 step 0000 | loss 0.0010\n",
      "epoch 98 step 0100 | loss 0.0002\n",
      "epoch 98 | train loss 0.0058 acc 0.9987 | val loss 2.0605 acc 0.7022 | best 0.7800 | 5.4s\n",
      "epoch 99 step 0000 | loss 0.0002\n",
      "epoch 99 step 0100 | loss 0.2524\n",
      "epoch 99 | train loss 0.0352 acc 0.9895 | val loss 1.5035 acc 0.7089 | best 0.7800 | 5.3s\n",
      "epoch 100 step 0000 | loss 0.0076\n",
      "epoch 100 step 0100 | loss 0.0377\n",
      "epoch 100 | train loss 0.0307 acc 0.9900 | val loss 1.5213 acc 0.6933 | best 0.7800 | 5.3s\n",
      "Saved best checkpoint to: /home/jovyan/projects/neuro-eeg/training_logs/labram_physionet_lr_allruns/best.pth\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "EPOCHS = 100\n",
    "PRINT_EVERY = 100  # steps\n",
    "\n",
    "def run_eval(model, loader, device, input_chans):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch[\"inputs\"].to(device, non_blocking=True)\n",
    "            y = batch[\"labels\"].to(device, non_blocking=True)\n",
    "\n",
    "            logits = model(x, input_chans=input_chans.to(device))\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            total_loss += float(loss.item()) * x.size(0)\n",
    "            n += x.size(0)\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_preds.append(preds.detach().cpu().numpy())\n",
    "            all_labels.append(y.detach().cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    acc = float((all_preds == all_labels).mean())\n",
    "    return total_loss / max(n, 1), acc\n",
    "\n",
    "best_val_acc = -1.0\n",
    "PATIENCE = 75\n",
    "min_delta = 1e-4  # require at least this much improvement\n",
    "no_improve = 0\n",
    "\n",
    "best_path = OUT_DIR / \"best.pth\"\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        x = batch[\"inputs\"].to(device, non_blocking=True)\n",
    "        y = batch[\"labels\"].to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x, input_chans=input_chans.to(device))\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # stats\n",
    "        running_loss += float(loss.item()) * x.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        running_correct += int((preds == y).sum().item())\n",
    "        running_total += x.size(0)\n",
    "\n",
    "        if step % PRINT_EVERY == 0:\n",
    "            print(f\"epoch {epoch:02d} step {step:04d} | loss {loss.item():.4f}\")\n",
    "\n",
    "    train_loss = running_loss / max(running_total, 1)\n",
    "    train_acc = running_correct / max(running_total, 1)\n",
    "\n",
    "    val_loss, val_acc = run_eval(model, val_loader, device, input_chans)\n",
    "\n",
    "    # save best\n",
    "    if val_acc > best_val_acc + min_delta:\n",
    "        best_val_acc = val_acc\n",
    "        no_improve = 0\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch,\n",
    "                \"val_acc\": val_acc,\n",
    "            },\n",
    "            best_path,\n",
    "        )\n",
    "    else:\n",
    "        no_improve += 1\n",
    "\n",
    "    if no_improve >= PATIENCE:\n",
    "        print(f\"Early stopping: no improvement for {PATIENCE} epochs. Best val_acc={best_val_acc:.4f}\")\n",
    "        break\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    print(\n",
    "        f\"epoch {epoch:02d} | \"\n",
    "        f\"train loss {train_loss:.4f} acc {train_acc:.4f} | \"\n",
    "        f\"val loss {val_loss:.4f} acc {val_acc:.4f} | \"\n",
    "        f\"best {best_val_acc:.4f} | {dt:.1f}s\"\n",
    "    )\n",
    "\n",
    "print(\"Saved best checkpoint to:\", best_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c6ab05-c8e5-4ff9-b065-d98e78460b36",
   "metadata": {},
   "source": [
    "## Test set evaluation (best checkpoint)\n",
    "\n",
    "We load the best checkpoint and evaluate on the held-out **test subjects**.\n",
    "We report:\n",
    "- test accuracy\n",
    "- confusion matrix (plotted + annotated)\n",
    "- per-class accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc804ae9-047a-4d35-b8d3-ca76228b6ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best checkpoint: /home/jovyan/projects/neuro-eeg/training_logs/labram_physionet_lr_allruns/best.pth\n",
      "TEST acc: 0.7554  (n=552)\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[198  81]\n",
      " [ 54 219]]\n",
      "class 0 (Left (T1)) acc: 0.7097  (n=279)\n",
      "class 1 (Right (T2)) acc: 0.8022  (n=273)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BEST_PATH = OUT_DIR / \"best.pth\"\n",
    "print(\"Loading best checkpoint:\", BEST_PATH)\n",
    "\n",
    "ckpt = torch.load(BEST_PATH, map_location=device)\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        x = batch[\"inputs\"].to(device, non_blocking=True)\n",
    "        y = batch[\"labels\"].to(device, non_blocking=True)\n",
    "\n",
    "        logits = model(x, input_chans=input_chans.to(device))\n",
    "        preds = logits.argmax(dim=1)\n",
    "\n",
    "        all_preds.append(preds.detach().cpu().numpy())\n",
    "        all_labels.append(y.detach().cpu().numpy())\n",
    "\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "test_acc = float((all_preds == all_labels).mean())\n",
    "print(f\"TEST acc: {test_acc:.4f}  (n={len(all_labels)})\")\n",
    "\n",
    "# Confusion matrix: rows=true, cols=pred\n",
    "cm = np.zeros((2, 2), dtype=int)\n",
    "for t, p in zip(all_labels, all_preds):\n",
    "    cm[int(t), int(p)] += 1\n",
    "\n",
    "class_names = [\"Left (T1)\", \"Right (T2)\"]\n",
    "\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\")\n",
    "print(cm)\n",
    "\n",
    "# Per-class acc\n",
    "for cls in [0, 1]:\n",
    "    mask = (all_labels == cls)\n",
    "    acc_cls = float((all_preds[mask] == cls).mean()) if mask.any() else float(\"nan\")\n",
    "    print(f\"class {cls} ({class_names[cls]}) acc: {acc_cls:.4f}  (n={int(mask.sum())})\")\n",
    "\n",
    "# --- Plot confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "im = ax.imshow(cm)\n",
    "\n",
    "ax.set_title(f\"LaBraM — PhysioNet EEGMMIDB\\nLeft vs Right (All runs) | Test acc={test_acc:.3f}\")\n",
    "ax.set_xlabel(\"Predicted label\")\n",
    "ax.set_ylabel(\"True label\")\n",
    "\n",
    "ax.set_xticks([0, 1], labels=class_names, rotation=20, ha=\"right\")\n",
    "ax.set_yticks([0, 1], labels=class_names)\n",
    "\n",
    "# Annotate cells\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\")\n",
    "\n",
    "# Colorbar\n",
    "cbar = fig.colorbar(im, ax=ax)\n",
    "cbar.set_label(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d4342-269e-40eb-a89f-a278dcebc2ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
